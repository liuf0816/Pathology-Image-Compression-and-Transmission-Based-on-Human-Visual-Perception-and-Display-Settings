Roadmap from version 6.1 to 6.2
-------------------------------
* Migrate all "kdu_server" and "kdu_client" code to a single relatively
  platform neutral code base, so that Windows, Linux, Unix and Mac
  deployments all have the same level of robustness and features.
* Enhance JPIP support for all platforms and add full JPIP support to
  the MAC viewer, "kdu_macshow", introduced in v6.1.
* Add a "kdu_vcom_fast" demo application, to demonstrate fastest-possible
  compression of video streams (analogous to "kdu_vex_fast" for decompression).
* Add some enhancements to the thread queue and I/O management processes in
  the Kakadu core system to improve utilization of machines with many CPU
  cores, without requiring developers to do a lot of hand tuning.  The goal
  is to get an out-of-the-box implementation like that found in "kdu_v_expand"
  to run almost as fast as a much more customized application like
  "kdu_vex_fast", with largish images and, say, 4 CPU cores.  To do this, the
  block decoding and transform processing engines need to be isolated more
  from the bottlenecks associated with compressed data I/O; I have a suitable
  approach in mind, which makes only small changes to the existing system, but
  will have to wait until v6.2.
* More extensive support for editing, rendering and serving JPX files with
  metadata links (via cross-reference boxes).  May possibly consider links
  into geo-referencing metadata as well -- access to georeferencing data is
  already supported, but there are no facilities to explicitly interpret it.

Changes from version 6.0 to 6.1
-------------------------------
* Provided a complete set of XCODE build environments for the MAC, to
  complement the existing Makefiles and Microsoft Visual Studio build
  environments.

* Dramatically improved the metadata editing capabilities of "kdu_show",
  while adding new options to save edited files and maintain as much
  metadata as possible in JP2 files (JPX is, of course, the preferred
  format to save images with rich metadata).

* Introduced a new viewing utility, "kdu_macshow", for the MAC (runs under
  OSX 10.5 on G4, G5 and Intel processors; should also run under OSX 10.4).
  This utility is more elegant than its long standing Windows cousin,
  "kdu_show".  It contains all the same features as "kdu_show" (with the
  exception of JPIP support, which will be enabled in v6.2), but adds
  automatic metadata cataloging and navigating features for JPX sources,
  manages multiple open windows and allows for synchronized commands to be
  delivered to multiple windows (e.g., start playing video in all windows at
  once).  It uses mostly the same accelerator keys as "kdu_show".
     It is worth mentioning that the "kdu_macshow" application is built using
  Kakadu's platform independent API's, together with Cocoa (basically,
  NextStep).  As a result, it seems likely that interested parties could port
  the application quite easily to Linux and other environments, via GnuStep.

* Improved JPX metadata management considerably, as follows (these features are
  all used by the new "kdu_macshow" application and, to a lesser extent by
  "kdu_show", for sophisticated metadata editing, navigation and integration,
  not previously offered by a Kakadu viewer).
  1. Introduced new member functions to `jpx_metanode' to make metadata editing
     more convenient.  These allow the type and contents of an existing node to
     be changed and the parent of a node to be changed.  Previously, these
     operations required sub-trees of the metadata hierarchy to be copied, in
     order to preserve all nodes other than the one that was being changed.
  2. Altered the internal operation of `jpx_meta_manager' so that metanodes are
     not physically deleted until the `jpx_source' or `jpx_target' object
     itself is destroyed.  This provides increased robustness to
     editing applications which might not take care of eliminating
     references to objects which have been deleted.  More importantly,
     it allows applications to explicitly retain references to nodes
     which have changed or been deleted and to discover such facts by
     calling `jpx_metanode::is_changed', `jpx_metanode::is_deleted' or
     `jpx_metanode::parent_changed'.
  3. Added a facility to keep track of original file locations (or
     copy source) associated with the metadata managed by
     `jpx_metanode' objects, providing the application with the
     ability to efficiently locate metanodes based on the original box
     locations (see `jpx_meta_manager::locate_node').  In the future,
     these same internal mechanisms will be used to recover the semantic
     associations (as links) implied by the use of cross-reference
     boxes and to preserve them through editing and copy operations so
     that interactive users can add, remove and navigate such links.
  4. Modified the code which reads metadata into a `jpx_source' object so
     that errors encountered in non-essential metadata can be non-fatal,
     just eliminating the affected metadata nodes from the generated
     metadata tree.
  5. Added a facility to allow applications to save an arbitrary state
     reference internally with each metadata node, to allow them to
     conveniently reconcile changes in the metadata structure (due to editing
     or JPIP delivery) with application-defined metadata structure.
  6. Added a service to efficiently and conveniently identify the set of
     metadata nodes which have been changed, deleted, added, or recently
     parsed into a `jpx_source' (e.g., because they became available in a
     dynamic cache, during JPIP browsing).  The application can now efficiently
     scan all such newly available nodes using the
     `jpx_meta_manager::get_touched_nodes' function.

* Modified the "kdu_compress" and "kdu_expand" applications to allow images
  with a given declared sample data precision to be read or saved as though
  they had a different sample data precision.  One reason for doing
  this is to overcome a weakness in the support offered
  by some third party TIFF reading/writing applications, in not properly
  supporting the packing/unpacking of sample values with non-power-of-two
  precisions.  Another reason for adding these features is that 16-bit TIFF
  files are commonly used to store data with substantially lower precision
  (e.g., only 11 or 12 bits), which will appear almost entirely "black"
  after regular compression with Kakadu (since all the values are close to
  the lower bound of the declared sample data range).  You can now conveniently
  instruct "kdu_compress" to treat the file as though the sample values had
  a lower precision.
     These modifications have nothing really to do with Kakadu
  proper, since TIFF file reading/writing in "kdu_compress" and "kdu_expand"
  is just for demonstration purposes, but a number of users requested these
  capabilities.  Be sure to carefully read the usage statement for the new
  `-fprec' (force precision) argument provided with each of "kdu_compress"
  and "kdu_expand".

* Slightly modified the summary data printing portion of the "kdu_expand" and
  "kdu_compress" demo applications so that they print large numbers
  (kdu_long's) in a nicer way.  This is achieved by augmenting the core
  `kdu_message' object with a special operator<< overload for the type
  "kdu_long".

* Provided a new core system feature to keep track of the number of compressed
  bytes in each quality layer, in each tile, image component and resolution.
  This can help applications to make intelligent choices regarding the number
  of quality layers they might choose to decode, where speed is particularly
  important.  The new feature centres around the function
  `kdu_tile::get_parsed_packet_stats', which comes with extensive comments
  to explain performance implications with different types of codestreams and
  compressed data sources.  The "kdu_expand" application is also augmented with
  a \"-stats\" argument to demonstrate the collection and printing of such
  parsed packet statistics.

* Augmented `kdu_codestream::flush' with the ability to impose constraints
  not only on the overall compressed size of each quality layer, but also on
  the size of the quality layer at each resolution and/or each leading subset
  of image components.  This is useful primarily for ensuring the generation
  of legal codestreams for Digital Cinema applications. The new feature is
  conveniently controlled via the codestream parameter system, using various
  flavours of the "Creslengths" parameter attribute.  This allows the feature
  to be added to just about any existing application without adding any
  (or hardly any) lines of code.  Along with this new feature, support for the
  Digital Cinema profiles CINEMA2K and CINEMA4K has been greatly upgraded,
  to include proper checking for legal digital cinema codestreams and
  automatic selection of suitable coding and structural parameters, where
  defaults must be supplied.  For example, you can now generate a legal 4K
  24fps digital cinema codestream using the following quite straightforward
  invocation of the kdu_compress demo app:
    >> kdu_compress -i in.tif -o out.j2c Sprofile=CINEMA4K \\
       Creslengths=1302083 \
       Creslengths:C0=1302083,1041666 \
       Creslengths:C1=1302083,1041666 \
       Creslengths:C1=1302083,1041666

* Significantly improved robustness of the core system to illegal or highly
  unusual conditions which might require the allocation of massive amounts
  of memory.  The core system should be able to correctly clean itself up
  (assuming the application calls `kdu_codestream::destroy') even after
  throwing an exception from a call to `new' which exceeds the available
  memory.

* Modified the function `kd_tile::read_tile_part_header' to check for
  tile-parts whose total length is 12 (an illegal value) and change it to
  14.  Lengths of 12 were accidentally recorded by Kakadu versions 6.0 and
  earlier, for empty tile-parts, when generating TLM information.

* Fixed the above-mentioned problem with creation of empty tile-parts with
  the illegal length value of 12 (changed to 14).

* Modified `kdu_resolution::get_precinct_packets' to take a `kdu_thread_env'
  pointer as its second (optional) argument, which is relevant only if the
  `parse_if_necessary' argument (previously the second, optional argument)
  is true.  This modification is deliberately intended to generate compilation
  errors for any application which was previously using the function in a
  manner which was potentially not thread-safe.  Read the interface description
  for more information.

* Fixed a bug in `kdu_codestream::trim_compressed_data' which has been in the
  core system since Part 2 arbitrary decomposition styles were introduced in
  version 5, but was only detected in version 6.0.  This bug could have
  potentially caused a memory access violation, but apparently hardly
  ever did so.

* Fixed a minor core system bug in `kd_thread_local.h', in which thread queue
  memory was released using `delete' rather than `free'.

* Fixed a long dormant bug in `kd_marker' which could manifest itself when
  a `kdu_codestream' object is restarted with a new set of coding parameters
  and an error is subsequently encountered in error resilient mode.

* Eliminated a potential race condition in the dereferencing of precincts
  which are candidates for recycled within the core system (one thread might
  recycle them while another is testing to see if they can be safely accessed)
  by using a volatile reference.

* Augmented the descriptions of `kdu_codestream::destroy',
  `kdu_codestream::share_buffering' and `kdu_codestream::open_block' to
  spell out the potential pitfalls associated with sharing buffering
  between multiple codestreams in multi-threaded mode (such as when one
  codestream is destroyed while another is still in use by a different
  thread).  Of course, applications which might do this are probably quite
  rare.

* Very slightly modified the return condition for
  `kdu_region_compositor::process' so that it adheres precisely to the
  explanation given in the header file (and hyperdoc API documentation).

* Fixed a more serious bug in `kdu_region_compositor', which caused it to
  generate a memory fault if a codestream, previously used to write alpha
  data for a compositing layer was recycled for use in single-component
  viewing mode.

* Fixed some bugs in the `jpx_meta_manager' and associated objects
  (i.e., `jpx_metanode').  Internally, there was a bug in the counting of
  descendants which could cause an error with some usages in the past.
  There was also an error in `jx_regions::write' which caused all ROI
  description boxes to be written with a height equal to the width.  These
  bugs have now been fixed.

* Fixed a bug in `kdu_client' (and the DSTO Unix/Linux port thereof) in which
  100-series responses (e.g., HTTP 1.1 "100 Continue" responses from
  intermediate proxies) were not properly passed over.

* Incorporated temporary fixes provided by a third party into the original
  DSTO Unix/Linux port of the "kdu_server" utility, so as to accommodate
  more than two clients.  The original Windows version of "kdu_server" is
  still the most robust implementation, until its methods get properly
  ported to a single more platform neutral realization (expected in
  Kakadu v6.2).

Changes from version 5.2.6 to 6.0
---------------------------------
* Added fast SIMD DWT and colour transformation code for the 32-bit
  precision sample processing path, which largely mirrors that already
  available in the 16-bit precision sample processing path.  This code
  can greatly accelerate high precision image compression/decompression,
  particularly at low bit-rates.  The accelerations are also available to
  DWT-based Part 2 multi-component transorms, which are more likely to
  require higher precision implementation.  These extra accelerations are
  available only on X86 platforms with SSE2 support or above.  Moreover,
  to gain access to the full set of accelerations, you should compile with the
  `KDU_X86_INTRINSICS' symbol defined.  This is automatic for 64-bit
  Windows/Linux/Mac compilations.
* Introduced additional methods on the core `kdu_thread' object, to support
  the assignment of thread priorities and the binding of threads to specific
  CPU sets (CPU affinity) in a platform independent way.  CPU binding can
  also now be optionally specified in calls to `kdu_thread_entity::create'.
* Added a new core codestream management feature which allows decompressed
  image quality to be traded for computational speed by stripping away final
  coding passes from selected code-blocks.  This has a similar effect to
  discarding quality layers, but works even when the original codestream
  was created with only one quality layer.  The mechanism is non-destructive
  so that the same codestream can be successively decompressed with different
  "truncation" policies, drawing from a single "persistent" `kdu_codestream'
  manager.  The feature is accessed via `kdu_codestream::set_block_truncation',
  which may be called at any point, allowing the quality/speed tradeoff to
  be managed dynamically, even while a single image is being decompressed.
  The feature may be demonstrated by the new "kdu_vex_fast" demo application,
  described below.
* Introduced several efficiency improvements to the management of
  compressed data in the core system, with the upshot that a speed
  improvement of several % is achieved when working with high bit-rate
  imagery, particularly if compressed data sources offer the new
  `KDU_SOURCE_CAP_IN_MEMORY' capability -- see the comments appearing
  with `kdu_compressed_source' for a full description.
* Substantially modified the `kdu_region_decompressor' object which is
  also the workhorse of the `kdu_region_compositor' object, so that these
  objects are both able to process horizontal strips of tiles together,
  in each call to their respective `process' functions.  This speeds up
  rendering of heavily tiled images, while the number of tiles processed
  together is still regulated so that limits on the amount of processing
  done in each call to `process' are respected.  For images with large
  tiles, the function still processes a user-controlled number of lines
  of one tile in each `process' call.  Along with this improvement in
  tile processing, the `kdu_region_decompressor' (and hence
  `kdu_region_compositor') object is now able to process multiple tiles
  in parallel, on machines with multiple CPU's (in addition to processing
  code-blocks within a tile in parallel); moreover, it is able to
  automatically start processing code-blocks in a new tile or strip of
  tiles once a current tile or strip of tiles nears completion so that
  some CPU's might otherwise become idle.
* Augmented the capabilities of the `jp2_input_box' and `jpx_input_box'
  objects to support pre-loading of their entire contents into memory
  via the new `load_in_memory' member function.  This method may be used
  to provide a compressed data source which offers the new
  `KDU_SOURCE_CAP_IN_MEMORY' option for the most efficient handling of
  compressed data throughout the core codestream management sub-system.
  The new capability is of principle interest for high performance
  video applications, including digital cinema.
* Augmented the "kdu_v_expand" demo application to offer a new "-in_memory"
  command-line option, which exercises the capabilities described in the
  above two bullet-points.  This allows you to explore the impact of
  in-memory compressed data sources on video rendering speed.
* Introduced a new demonstration application, "kdu_vex_fast", whose purpose
  is to demonstrate the fastest possible means of rendering JPEG2000
  compressed video content for real-time applications, including
  software-only digital cinema.  When compiled on Win64 and Win32 platforms,
  this application provides a DirectX 9 display interface with real-time
  frame rate control for high quality tear-free display.  If you don't
  have DirectX 9 support on your platform, you should undefine "KDU_DX9"
  when compiling this application.  When you do this, or when you compile
  under GCC, you get everything except display.  The application can
  write decompressed data to VIX files, exactly like "kdu_v_expand".  Also,
  as for "kdu_v_expand", omitting the output file from the command line
  argument allows you to assess the true decompression speed, without
  the limitations of file writing -- no other steps are skipped when you
  omit an output file name.  This application uses in-memory data sources
  and provides you with the flexibility to choose how processing threads
  will be distributed between frame processing engines and whether or
  not they should be tied to specific CPU's in NUMA environments -- see
  the "-usage" statement under the heading "-engine_threads" for more on this.
* Included a Unix/Linux port of the "kdu_server" utility, based on a port
  originally provided by Australia's Defense Science and Technology
  Organization (DSTO).  In the future, this port may be absorbed into
  the regular "kdu_server" application, but for now the ported files and
  relevant Make files are located under the separate "contrib" directory.
* Modified the implementation of `kdu_rijndael' in "kdu_security.cpp", fixing
  a minor error in the interpretation of the AES standard (not that it really
  matters for the way Kakadu uses AES) and correcting a platform byte order
  dependence, so that "kdu_server_admin" and "kdu_server" can interact
  successfully across different platforms; this is important now that the
  server can be compiled to run on Sparc and other processors which use a
  big-endian byte order.
* Modified "kdu_hyperdoc" to correctly cross-reference all global
  constant definitions, so as to make their accessibility and use more
  obvious, particularly for Java and C#/VB developers.
* Modified the way "kdu_hyperdoc" builds the "kdu_jni.cpp" source file
  for Java bindings, so that the class loading code is protected by a
  global mutex.  It is unclear whether any race conditions have ever
  been observed within the 3 machine instructions where the class loading
  code is potentially vulnerable; however, the mutex protection should
  guarantee that such race conditions are not possible.
* Added two new low level classes `kdu_compressed_source_nonnative' and
  `kdu_compresssed_target_nonnative' which can be inherited by foreign
  language classes (typically Java and C#) to provide fully custom
  compressed data sources and targets in those languages -- e.g., memory
  mapped compressed data sources/targets.
* Modified the make "managed/make" makefiles so as to put all
  relevant kakadu object code into a single shared library for both
  "libkdu_jni.so" and "libkdu_a60R.so", so that these libraries can
  be imported more reliably by other applications, such as JVM's (for
  the JNI bindings).  Moreover, special precautions are now taken to
  ensure that these libraries do not use X86 compiler intrinsics on 32-bit
  Linux platforms, due to the stack alignment problems which can arise.
  At the same time, though, the regular applications built with GCC do
  get to benefit from the most comprehensive collection of processor
  speedups, which are implemented via the X86 compiler intrinsics.
  The -fPIC option is now used on all relevant builds, which should also
  maximize portability.
* Created top-level build environments for Linux, MAC and Solaris operating
  systems, which build all applications, libraries and managed interfaces
  in one hit.  These are found under the new top-level "make" directory.
  Moreover, the Makefile-MAC-x86-all" makefile builds both 32-bit and 64-bit
  binaries for Intel MAC's and then joins the 32-bit and 64-bit JNI library
  into a unified library which can be used from both 32-bit and 64-bit
  JVM's -- this is done in preparation for the arrival of Leopard, in case
  it has a 64-bit Java Virtual Machine.
* The "managed/make" makefiles and the .NET build environment in the
  "managed" directory all now find the base location of the Java SDK
  by expanding an assumed environment variable "JAVA_HOME", which may
  well already be defined on your system.  This saves you having to
  modify the build environments for Java with each new Kakadu release.
  See the "Compilation_Instructions.txt" file for more on "JAVA_HOME".
* Added a "-com" command-line argument to the "kdu_compress" demo application
  which allows one or more user-supplied COM marker segments to be inserted
  into the codestream.
* Added a "-cpu" option to "kdu_render", which can be used to properly
  evaluate the computational throughput of the `kdu_region_decompressor'
  object, which is central to image rendering with Kakadu.  The speed of
  this object is identical to that of `kdu_region_compositor' for simple
  (non-composit) images.
* Fixed a subtle bug in the core system, which could have caused
  spurious error messages during codestream parsing (e.g.,
  "Illegal inclusion tag tree encountered ...").  This problem was
  occasionally reported with large tiled imagery, in conjunction with
  "kdu_server", but not previously resolved.  The errors were caused by the
  interaction of one non-fatal bug with one subtle oversight.  The non-fatal
  bug is that recycled typical tile resources did not have their
 `kd_precinct_pointer_server' object's re-initialized so that random
  access packet length information in the codestream was ignored.  This
  left the situation where some tiles could have seekable precincts, while
  others do not -- something that could happen anyway in legal, but
  unreasonable codestreams.  The fatal problem occurred when random
  access was made to precincts of one tile, while another purely sequential
  tile was still being actively parsed.  This type of problem could have
  been excited by the "kdu_show" or "kdu_server" demo applications only.
* Fixed a bug in "kdu_threads.cpp", which caused any dormant queues to be
  incorrectly shutdown by calls to `kdu_thread_entity::terminate' and
  `kdu_thread_entity::destroy'.
* Fixed a subtle bug in the core system functions `kd_tile::initialize'
  and `kd_tile::recycle', in which `sum_depths' was downshifted by 2 instead
  of 1, leading to incorrectly computed quantization step sizes for
  irreversible processing with "Cderived=yes".  Interestingly, this bug
  virtually never caused incorrect decompression, since an exact power of
  2 error in the quantization step size is always compensated by a
  corresponding incorrectly computed K_max value, representing the number
  of nominal bit-planes in a subband's code-blocks.  Moreover, the error
  can only occur in the higher frequency subbands.  Nevertheless, this bug
  may have caused incorrect treatment of ROI regions if the "Max Shift"
  ROI encoding method had been used with derived quantization.  This bug
  was accidentally introduced with the introduction of Part-2 quantization
  kernels and found by Roddy Shuler.
* Fixed a bug in the implementation of the `kdu_event' platform-independent
  synchronization object, for the case of auto-reset events on pthreads
  platforms (i.e., Unix/Linux and Mac OS) -- this problably had no impact
  on pre-existing applications.
* Identified and resolved another obscure bug with the way in which
  non-symmetric DWT kernels (only allowed in JPEG2000 Part-2) are handled
  in the case of certain boundary conditions.
* Fixed two non-compliance problems with the way Kakadu handles
  Part-2 multi-component transforms.  The first problem relates to an
  oversight in previous versions of Kakadu, whereby the presence of
  matrix- and/or dwt-based multi-component transforms was not signalled
  in the COD marker segment -- this is an entirely redundant signalling,
  but required by Part-2 of the JPEG2000 standard.  The second problem
  is that previous versions of Kakadu stored the transform coefficients
  for reversible matrix-based (SERM) multi-component transforms in a
  transposed order from that specified by the standard; the problem in
  this case originated from the fact that the main figure in
  Annex J of IS15444-2 suggests the transposed order, whereas the true
  order is only revealed by the order of subscripts in the equations.
  To fix these compliance problems, while remaining as compatible as
  possible with past versions of Kakadu, the following steps have been
  taken:
  1) A new `Cmct' coding parameter attribute has been introduced to the
     `COD_params' object, which reflects the required MCT-dependent
     modifications to the COD marker segment for Part-2 codestreams.  This
     parameter attribute is automatically generated during finalization
     of the `COD_params' object(s) so you don't need to explicitly worry
     about it.
  2) A new `MATRIX' option has been created for the `Mxform_blocks'
     attribute, to be used in describing matrix-based multi-component
     decorrelation transforms, in place of the old `MAT' option.  In source
     code, these correspond to `Cxform_MATRIX' and `Cxform_MAT', respectively.
     You should no longer use `Cxform_MAT'.  If you have an existing
     application which uses `Cxform_MAT' with Kakadu, it will continue
     to compile successfully, but you will receive an informative error
     message when you attempt to generate a codestream using this option.
     This is our way of making you aware of the need to transpose any
     reversible multi-component matrix transform coefficient array that
     you might be using in an application compiled against previous
     versions of Kakadu.
  3) Codestreams generated by previous versions of Kakadu, including
     reversible multi-component matrix decorrelation transforms can still
     be successfully decoded; indeed, the relevant transform blocks will
     show up as the old type `MAT' (`Cxform_MAT') in place of the new
     `MATRIX' (`Cxform_MATRIX') if you inspect the coding parameter
     attributes explicitly after the codestream has been ingested.  Kakadu
     detects the presence of the old-style non-compliant conventions by
     observing the absence of the `Cmct' parameter attribute when a
     multi-component transform is being used.  It should be noted, however,
     that other applications (other than Kakadu) are unlikely to correctly
     decompress codestreams generated by previous versions of Kakadu
     which incorporated reversible matrix-based multi-component transforms.
* Fixed a bug in the `kdu_thread_env' core multi-threading framework which
  allowed race conditions when multiple output codestreams are repeatedly
  opened and closed, with a single multi-threaded environment processing
  those codestreams.  Along the way, the implementation has been improved
  so that it is no longer strictly necessary to terminate all thread queues
  via the `kdu_thread_entity::terminate' function before destroying any
  codestream, so long as all work on that codestream has ceased -- in
  particular, this means that it is sufficient to wait for just those
  thread queues which are associated with a codestream to terminate, while
  other threads may be getting on with processing other codestreams; this
  is true for input, output and interchange codestreams.
* Fixed a bug in "kdu_serve.cpp" which could produce a divide-by-zero error
  for certain illegal JPIP requests, thereby compromising the integrity of
  the server application.
* Fixed another bug in "kdu_serve.cpp" which prevented JPIP "metareq"
  requests which contain box-type wildcards (*) from being correctly processed.
* Fixed a bug in `kdu_client' which caused the wrong JPIP syntax to be
  used for meta-data-only requests -- the server was changed to support the
  correct syntax back in early 2006, but the corresponding fix in the client
  was long overlooked.
* Also fixed an error in the way `kdu_client' communicates with HTTP
  proxies, so as to properly conform to HTTP/1.1 conventions.
* Fixed a bug in the TIFF reading code used by the "kdu_compress" demo
  application, which causes LZW compressed tiled images to be read
  incorrectly.  Problem reported by Greg Coats, with fix provided by
  Margaret Lepley.
* Fixed a bug in the server delegation code in Kakadu's "kdu_server" app.
* Fixed a bug in the implementation of `kdu_window::contains' so as to
  avoid possible erroneous treatment of novel client windows as non-novel
  in either the server or the client.
* Fixed a couple of bugs in rarely used data processing paths within
  `kdu_region_decompressor', which may have affected applications with
  high rendering bit-depth requirements.

Changes from version 5.2.5 to 5.2.6
-----------------------------------
* Fixed a minor bug in `tif_in::tif_in' within "image_in.cpp" which
  caused Planar Configuration TIFF files to be misread on some
  platforms, depending on how uninitialized member variables of
  classes are treated.
* Fixed a bug in `kdu_region_compositor::find_point' which could cause
  dereferencing of a NULL pointer when trying to match a screen location
  to a codestream during JPIP browsing, when the server has sent enough
  information to identify the compositing layer, but not enough information
  to identify its codestreams.  This is a very rare condition, but
  could be excited in the "kdu_show" application by depressing "ctrl"
  in the early phases of browsing a large JPX photo album.
* Made the destructor for `kdu_cache' virtual -- this was an oversight,
  since derived objects do have virtual destructors.
* Extended the cases under which tiles are considered "typical" for the
  purpose of recycling their structures.  This helps speed up the
  decompression of heavily tiled images at reduced resolution, where
  each tile can be very small.  The new conditions help with cases
  where the tiles may have different quantization or ROI parameters.
  In the process, a potentially bug-prone condition was uncovered
  and fixed.

Changes from version 5.2.4 to 5.2.5
-----------------------------------
* Fixed some backward compatibility problems with Visual Studio 6
  in the way two new message handlers were added by .NET to the
  "kdu_show" demo app.
* Fixed some minor problems with the "managed/managed_2005.sln" workspace
  and related projects, for Visual Studio 2005 users.
* Fixed a minor initialization problem in the "kdu_stripe_compressor" and
  "kdu_stripe_decompressor" objects, which could impact applications
  relying on the `get_recommended_stripe_heights' function.

Changes from version 5.2.3 to 5.2.4
-----------------------------------
* Fixed a minor bug in "kd_tcp_transmitter::configure_flow_control"
  which caused the function's arguments to be ignored.  This function is
  used to adjust min/max RTT times based on command-line instructions to
  the server.
* This version comes with separate build environments for Visual Studio
  .NET 2003 and Visual Studio .NET 2005, the latter including both Win32
  and Win64 build configurations.  Some very minor changes were introduced
  into the code to eliminate errors/warnings during Win64 builds.
* The "kdu_hyperdoc" utility has been upgraded to allow conformance with
  either the older style (V1) or the newer style (V2) syntactic conventions
  for Microsoft's Managed Extensions to C++.  The newer style conventions
  are used by default, but these are appropriate only when building with
  Visual Studio 2005.  To access the older style conventions, for builds
  under Visual Studio 2003, use the "-old_managed_syntax" argument.  If
  you use the standard build environments ("managed_2003.sln" or
  "managed_2005.sln") everything is built automatically for you, using
  the appropriate conventions.
* The multi-threading environment managed by `kdu_thread_env' now comes
  with support for the maintainance of initially dormant queues, which
  are automatically moved into the foreground for processing once the
  system seems to have entered a state in which the available processing
  threads will otherwise be permanently under-utilized.  This can be used
  to build multi-threaded applications which keep all physical processors
  active even more of the time than with previous versions (which were
  already very good at utilizing available processing resources).  The
  new capability is described in connection with the optional
  `bank_idx' argument accepted by the `kdu_thread_entity::add_queue' function.
  So far, this capability is exploited only by the "kdu_v_expand" demo
  application, which offers a new "-overlapped_frames" command-line
  argument.  Try playing around with various combinations of the
  "-overlapped_frames" and "-double_buffering" arguments on your
  multi-processor system (particularly if it has more than 2 CPU's).

Changes from version 5.2.2 to 5.2.3
-----------------------------------
* Modified the CPUID testing code in "kdu_arch.cpp" yet again, this time
  so as to protect the EBX register on x86 platforms, since that register
  is reserved for position independent code when compiling with the "-fPIC"
  option under GCC, and marking it as a clobber variable does not work in
  that context.
* Modified the tile-part header marker segment reading code in
  "kdu_compressed.h" to ignore tile-part numbers, since Adobe encoders
  get them wrong, causing premature termination of the decoding process
  in compliant decoders.
* Modified the `kdu_thread_env' thread management code to allow multiple
  codestreams to be processed simultaneously by a single Kakadu multi-theaded
  environment.

Changes from version 5.2.1 to 5.2.2
-----------------------------------
Minor changes as follows:
* Arranged for .NET build environments to write the debug ".pdb"
  files associated with the core system (kdu_v52D.pdb) and
  managed DLL's (kdu_a52D.ddb, kdu_jni.pdb and kdu_mni.pdb) into
  the "bin" directory instead of the temporary "v5_generated"
  directory, so as to facilitate debugging from other workspaces.
* Added the "KDU_AUX_EXPORTS" to the virtual functions which are
  offered by the "kdu_a52?.dll" auxiliary DLL, in addition to the
  existing non-virtual functions.  This allows Windows users to
  access these functions directly, rather than having to indirectly
  access them through function pointers.  No impact on Unix builds.
* Corrected the source of compilation warnings in GCC regarding
  base functions hidden by derived objects.
* Corrected and expanded the set of foreign language member access
  functions provided for the `kdu_sampled_range' object.
* Added an optional argument to `kdu_region_compositor::set_buffer_surface',
  allowing you to set (or change) the background colour (and transparency)
  of the rendering surface onto which imagery is composited.  The value
  is relevant only when imagery does not fully cover the compositing
  surface or when it is partially transparent.  The default (backwards
  compatible) value is an opaque white background.  You can use this
  new feature to blend rendered imagery onto another buffer managed
  by the application.
* Added `add' and `subtract' member functions to the `kdu_coords' object,
  as substitutes for the `operator+=' and `operator-=' functions which
  cannot be exported to foreign languages.

Bug fixes as follows:
* Fixed a bug in "kdu_server" which could cause the server
  to hang when used with delegation over HTTP transport channels.
* Fixed a bug in the core system which could cause a crash in the
  event that a tile had only empty tile-parts, when working in
  random access mode with seekable codestreams.
* Fixed a minor bug in `kdu_region_decompressor' caused by
  `post_convert_colour' and `pre_convert_colour' not always being initialized.
* Modified the test inside an "assert" statement found inside
  `kd_decoder::init' and `kd_encoder::init' to allow for subbands of 0
  width in multi-threaded processing -- no impact on release code used in
  final applications.
* Fixed a couple of bugs in "kdu_threads.cpp" which manifested themselves
  only when multiple synchronization conditions were simultaneously
  installed in a multi-threaded system.  This condition typically only
  happened when incremental flushing was used with a multi-threaded
  compressor, so that the synchronized flush worker job and the synchronizing
  event associated with closing down a tile processing engine could be
  registered simultaneously (depending on the circumstances).  This and
  related problems should now all be fixed.
* Fixed a minor bug in the incremental flushing routine implemented in
  the `kdu_stripe_compressor' object, which manifest itself only in
  multi-threaded environments.  The implementation has also now been
  made more efficient.
* Fixed a minor bug in the use of the `Kdu_coords.Minus' function in
  "KduRender2.java" and "KduRender2.cs" (Java and C#) demo apps, which
  affects imagery not centred at the origin.
* Made minor fixes to the code executed during Windows compilations with
  KDU_NO_SSE defined.

Changes from version 5.2 to 5.2.1
---------------------------------
This is just a bug fix release, as follows:
* Fixed a bug in "kdu_hyperdoc" which caused it to generate duplicate
  delete statements for local copies of size-incompatible buffers
  passed across JNI interfaces.
* Fixed a bug in the `kdu_region_decompressor' object which could
  cause it to crash when non-initial components are used in isolation
  from a codestream.  This bug was accidentally introduced in v5.2 by code
  which tests for the usability of some accelerated buffer manipulation
  paths.
* Fixed a remaining bug in Kakadu's server delegation feature, for the
  HTTP-TCP protocol.  The bug was in the way the `kdu_client' object
  processed notifications of server address changes.
* Fixed a couple of GCC complaints in relation to virtual objects with
  non-virtual destructors -- these were for objects which have no
  real destructors and are never actually derived.
* Fixed remaining compiler warnings for WIN64 builds.
* Modified the implementations in the X86 intrinsics headers to provide
  pure SSE/SSE2 implementations of all processor speedups, with a compiler
  switch to disable all of the older 64-bit MMX speedups.  This is done
  to cater for WIN64 builds which refuse to link 64-bit MMX instructions
  (possibly because WIN64 does not save 64-bit MMX state during context
  switches -- 64-bit LINUX has no such difficulties mixing SSE/SSE2 and MMX).
* Fixed the Macintosh makefiles (for both PowerPC and X86 processors) so
  that they build dynamic rather than static libraries, and hence can be
  immediately used with Kakadu's Java native interfaces.  The new makefiles
  were developed in collaboration with Greg Coates.
* Fixed the feature checking code in "kdu_arch.cpp" for GCC builds on
  X86 platforms -- the assembly code there previously clobbered several
  registers without letting GCC know, which sometimes caused problems in
  the initialization sequence.  In some cases, this might have led to
  GCC builds not recognizing MMX/SSE/SSE2 support, so that the SIMD
  optimizations would not be applied.
* Fixed a minor source of numerical inaccuracy in the fixed-point
  implementation of irreversible wavelet kernels when SIMD processor
  speedups are not available -- not very often, seeming as they are
  available for X86, PowerPC and Sparc processors.

Changes from version 5.1.1 to 5.2
---------------------------------
* Previous deficiencies of Kakadu on Win64 systems have been corrected.
  Most importantly, the .NET compiler cannot use Microsoft inline assembly
  for 64-bit builds, which had forced developers to disable the MMX/SSE/SSE2
  speedups provided by Kakadu when building 64-bit systems.  This problem
  did not exist for 64-bit Unix builds.  The problem has been remedied
  by providing new implementations of the SIMD speedup code using
  processor intrinsics.  These are contained in header files with names
  of the form "x86_xxx_local.h".  Conditional compilation logic selects
  the most efficient, compliant implementation based on the targeted
  machine.  For more information, see the "Compiling_Instructions.txt"
  file, which has been substantially revised.
* Additional makefiles have been provided for MAC systems with Intel
  processors.  Also, the makefile naming conventions have been revised
  for increased clarity, as have the names of the separate directories
  into which they write their results, sitting under the "lib" and
  "bin" directories.  The new names should be self-explanatory.
* "kdu_hyperdoc" now builds C# and Visual Basic interface bindings, along
  with a more comprehensive set of Java bindings.  All of these interface
  bindings and corresponding foreign language examples are now found in
  the new "managed" directory, which contains makefile, MSVC and .NET
  build environments to build everything for you -- after building and
  running "kdu_hyperdoc" from the "apps" directory in the usual way.
* One new Java application example and two C# application examples
  have been provided in the "managed/java_samples" and
  "managed/csharp_samples" directories.  The examples are in one-to-one
  correspondence between the two languages.  They demonstrate use of
  the "kdu_region_decompressor" object and now the more generic
  "kdu_region_compositor" object to render images in these languages.
* "kdu_hyperdoc" now creates directories as required, if they do not
  already exist.  This should avoid problems encountered by new users
  who do not have the correct directory layout configured on their system.
* "kdu_hyperdoc" now copies all public API header files to a common
  location, "managed/all_includes" for convenience of application
  developers.  It also writes an auxiliary DLL / shared library
  named "kdu_a52R.dll" (debug version "kdu_a52D.dll") / "libkdu_a52R.so",
  which includes all useful generic classes from the "apps" directory.
  You can simply include this DLL/shared library along with the core
  system DLL/shared library ("kdu_v52R.dll", "kdu_v52D.dll" or
  "libkdu_v52R.so" as appropriate) to access all of the functionality
  declared in the headers in "managed/all_includes".  To build this
  auxiliary DLL/shared library, use the makefiles or Microsoft
  compiler build environments found in the "managed" directory.
* "kdu_hyperdoc" now allows you to explicitly specify which classes,
  global functions or even member functions you would like to receive
  Java, C# or Visual Basic language bindings via the new "-bind"
  option.  Of course, if no "-bind" argument is used, you will get
  bindings for the whole lot, as before -- actually more than before.
* `kdu_region_decompressor', `kdu_region_compositor' and all things
  which depend uon them now perform premultiplied alpha blending, in
  addition to regular alpha blending which has been available for
  a very long time.
* `kdu_region_decompressor' provides a new mode-setting function,
  `set_white_stretch', which may be used to control how low bit-depth
  imagery is rendered into higher bit-depth buffers.  In particular,
  the use of this option allows you to ensure that low bit-depth
  images will exactly span the maximum dynamic range of 0 to 2^{B-1}
  associated with a B-bit rendering buffer, at the expense of some
  computation.  The function can set a threshold at which stretching
  happens, so that very few cases incur the small computational
  increment.  The default value of this threshold in `kdu_region_decompressor'
  ensures exact backward compatibility with previous versions of the object,
  but `kdu_region_compositor' sets the threshold for optimal rendering
  to 8-bit/sample displays.
* The "kdu_server" application has been augmented with a "-cd"
  command-line option, which allows you to select a different directory
  for storing the ".cache" files which are created to ensure consistent
  serving of image files.
* A nice summary of all compilation directives now appears in the
  "Compiling_Instructions.txt" file, which has been substantially
  restructured to make things more accessible.
* Bug fixes as follows:
  -- In "kdu_serve.cpp", used by the "kdu_server" JPIP server application,
     a minor bug in the computation of bytes associated with previously
     served packets was corrected.  Specifically, in function
     `kd_serve::simulate_packet_generation', the line
       "for (cum_packets=1; cum_packets < tp->num_layers; cum_packets++)"
     is changed to
       "for (cum_packets=1; cum_packets <= tp->num_layers; cum_packets++)"
  -- In "kdu_hyperdoc", an earlier fix for a bug in the source parsing
     code accidentally caused bare functions (i.e., functions without
     classes) to be omitted from the automatically generated documentation
     and in the construction of Java native interfaces.  This has now
     been created.
  -- In "kdu_region_decompressor" in function `interpolate_and_convert',
     an unlikely condition which could cause the final column
     of a rendered region to be unwritten, in the event that the fast
     SIMD speedups are used, has been corrected.
  -- Fixed a bug in the "kdu_server" application's delegation feature,
     which has been present for some time but manifested itself only
     when the delegated server has a different IP address to the delegating
     server (as opposed to just a different port number).  The cause of the
     problem was a single typo, where the delegating server used syntax
     "hostname=" instead of the correct JPIP syntax "host=" which is
     expected by the client.
  -- Fixed a bug in the core coding parameter management system, which
     has been in existence since version 4.2, where sparse parameter
     instantiation on the tile-component grid was introduced in order to
     dramatically cut the cost of parameter management for heavily tiled
     (or componented) images.  Unfortunately, the instantiation logic did
     not pick up on the fact that separate instances of the quantization
     parameter object `qcd_params' need to be instantiated for image
     components whose precision, as recorded in the SIZ marker segment,
     differs from that of the first component, when reversible compression
     is involved, due to dependencies between precision and reversible
     dynamic range parameters.  The problem was very easy to fix by
     slightly modifying `qcd_params::finalize'.

Changes from version 5.1 to 5.1.1
---------------------------------
* Minor features added as follows:
  -- Upgraded the `jp2_colour_converter' object to allow for the conversion
     of 4 colour spaces to RGB, which means that "kdu_show" and other
     tools now automatically handle rendering of CMYK to RGB.
  -- Significantly modified the TIFF reading and writing code used by the
     demo applications kdu_compress and kdu_expand, so as to correctly handle
     a much wider range of TIFF files.  The code now handles both tiled and
     untiled images, with both planar and contiguous pixel organizations.
     In addition, a bug in the palette handling was removed -- this but
     was accidentally introduced in v5.1 when generic baseline TIFF handling
     was provided through native Kakadu tools.  TIFF reading and writing is
     still not really considered part of the Kakadu toolset, since it is
     used only for demonstration purposes; however, a lot of people have
     asked specifically for this, so they can more conveniently use the
     demo apps.
* Bug fixes as follows:
  -- The "kdu_tiff" object introduced in version 5.1 contained a
     subtle yet important bug when handling tags whose value is
     rational, where the file byte order differs from that of the
     native machine.  The fix involved restricting the swapping
     of 4-byte words to double precision floating point data types,
     rather than all 8-byte data types, as suggested by Michael
     Wildermoth.
  -- Two fixes to the raw file reading code in "kdu_compress", both
     suggested by Margaret Lepley.
  -- Removed the second writing of TIFF tag RowsPerStrip when
     generating GeoJP2 boxes in "image_in.cpp", which was a typo.
     This fix was provided by Greg Coats.
  -- Removed a typo in the call to `_addr_to_kdu_int32' in
     `simd_upshifted_interleave' within "gcc_dwt_altivec_local.h".
  -- Fixed a memory leak in `jpx_codestream_source::access_dimensions'
     where the `kdu_codestream' object which was created by this function
     in order to completely finalize compatibility information, was not being
     destroyed.
  -- Fixed a foolish error in `kd_serve::process_window_changes' where
     a `kdu_window' object was directly assigned to another rather than
     using the `kdu_window::copy_from' function.  This caused `kdu_server'
     to crash on connection closure, which is a behaviour that should have
     been caught prior to release of v5.1 were it not for the fact that the
     bug was introduced immediately prior to release, as a fix for another
     much more subtle error.
  -- Modified the colour space interpretation code in "kdu_compress" so
     as to allow proper representation of 4-colour spaces such as CMYK --
     of course, this is just a demo, but it's nice for the demo to be general.

Changes from version 5.0 to 5.1
-------------------------------
* The main new feature in Kakadu v5.1 is the inclusion of extensive
  multi-threading facilities to exploit the computing resources
  available on multi-processor, multi-core and hyperthreading platforms.
  This is achieved through two new core system objects, `kdu_thread_entity'
  and `kdu_thread_env'; the latter is derived from the former.  You can
  completely ignore these objects, compiling your applications exactly
  as before, in which case only one thread will be involved in processing.
  Alternatively, the simplest way to reep advantages on multi-threaded
  platforms, is to create a `kdu_thread_env' object and pass it (as an
  optional argument) to the objects you are using to perform your
  processing.  All of the high level sample processing objects
  (`kdu_multi_analysis', `kdu_multi_synthesis', `kdu_stripe_compressor',
  `kdu_stripe_decompressor', `kdu_region_decompressor' and
  `kdu_region_compositor') provide simple mechanisms to include the
  processing resources offered by a `kdu_thread_env' object you have
  created.  You control the number of working threads by using the
  `kdu_thread_entity::add_threads' function.  In most cases, you should
  arrange for the total number of working threads to be equal to the
  number of distinct real or virtual processors -- a value which Kakadu
  attempts to find for you via its `kdu_get_num_processors' function.
     Of course, there are lots of lower level ways to use the multi-threading
  framework, but the high level objects will be simplest to understand
  and require only a few extra lines of code.  To learn more about how
  to program with Kakadu's multi-threading environment, consult the
  demo applications -- "kdu_render", "kdu_compress", "kdu_expand",
  "kdu_buffered_compress", "kdu_buffered_expand", "kdu_v_compress",
  "kdu_v_expand" and "kdu_show" all provide facilities to use (or not use)
  the multi-threading environment and to control the number of threads used
  (typically a `-num_threads' argument).
     There is a lot of scope to do more with multi-threading than the demo
  applications can illustrate.  For example, the "kdu_v_expand" and "kdu_show"
  applications process each frame in a video sequence completely
  (synchronizing on the completion of all threads) before moving on to a
  new one.  This incurs some start-up and close-down costs where only one
  thread can execute.  It is possible, however, to arrange for overlapped
  processing of multiple frames.  This may be demonstrated explicitly
  in future versions of Kakadu.  Depending on the number of processors
  available on your platform, you may also find it useful to play around
  with double buffering options -- see, for example, the `-double_buffering'
  options to "kdu_compress" and "kdu_expand", which are used to enable and
  tweek double buffering features offered by `kdu_multi_analysis' and
  `kdu_multi_synthesis' to parallelize the processing of multiple
  tile-components.
     Use of the multi-processing framework with multiple threads incurs a
  memory overhead for the double buffering of code-block samples, since
  block encoding and decoding operations provide the greatest
  opportunity for parallelism in JPEG2000.  The extra memory is allocated
  automatically, using an algorithm which backs away from full double
  buffering as the image dimensions become very large.  For very large
  single-tiled images, the memory penalty associated with multi-threaded
  processing reaches around 30%.  If you are interested in playing with
  the internal algorithm which controls these costs, take a look at the
  `kd_encoder::init' and `kd_decoder::init' functions -- in particular,
  you may adjust the way in which the `num_jobs_per_row' and
  `buffer_height' member variables are initialized for each subband's
  `kdu_encoder' or `kdu_decoder' object.
     During compression, it is possible to overlap incremental codestream
  generation with ongoing processing, when using Kakadu's incremental
  flushing features.  This is demonstrated in "kdu_compress" and is also
  implemented for you within `kdu_stipe_compressor'.  It is achieved by
  defining so-called "synchronized jobs", which are scheduled at the first
  convenient opportunity after a synchronization condition is reached
  (typically, the processing of all samples pushed into the DWT engines
  so far).  The positioning of synchronized flushing jobs for maximum
  parallelism is a little tricky, so it is best either to use
  `kdu_stripe_compressor' or to read the explanation appearing within the
  "kdu_compress" function `compress_multi_threaded'.
     One nice feature of Kakadu's multi-threading environment is that you
  can compile and test applications based on the `kdu_thread_env' and
  `kdu_thread_entity' objects even if your platform does not support
  multi-threading.  To compile without actual multi-threading support,
  define the `KDU_NO_THREADS' symbol.  All that will happen in this case
  is that attempts to add additional threads to a `kdu_thread_env'
  (equiv. `kdu_thread_entity') object which you create will fail, leaving
  you with only one thread of execution.  This single thread (the one
  your program started in) will then be scheduled onto the various tasks
  automatically, as required.  The same thing happens if you never add
  extra threads to a `kdu_thread_env' object before using it.
    For true multi-threading, the Kakadu implementation supports both
  POSIX threads (Pthreads) and Windows threads.  If neither is available
  on your platform, you should define the `KDU_NO_THREADS' symbol, as
  mentioned above.

* Added new `push_stripe' and `pull_stripe' interface functions to the
  `kdu_stripe_compressor' and `kdu_stripe_decompressor' objects, respectively,
  to support 32-bit integer and floating point image sample values, in
  addition to the existing 8- and 16-bit precision interfaces.  These
  additional interfaces ensure that the high level stripe-oriented
  objects support all of the data precisions that are supported by the
  underlying core Kakadu system.

* Added the Digital Cinema profiles (CINEMA2K and CINEMA4K) to the list
  of profiles recognized in the codestream SIZ marker segment.  These
  profiles were added about a year ago as an ammendment to Part 1 of
  the JPEG2000 standard.

* Added significant support for TIFF and GeoTIFF image I/O to "kdu_compress"
  and "kdu_expand".  Even though image I/O is not really part of the scope
  of Kakadu, it is required for demonstration purposes and lack of
  comprehensive TIFF support has been a source of concern for some new users.
     This has been complicated by the fact that GeoJP2 files include a JP2
  box which is really an encapsulated TIFF file.  To solve both problems in
  one go, this new release of Kakadu comes with a simple yet general native
  TIFF parser/creater, which is not based on any external libraries and
  integrates well with the other abstract I/O services endemic to Kakadu.
  As a result, TIFF files can now be read and written by the "kdu_compress"
  and "kdu_exand" demo applications without the need to link against LIBTIFF
  (as before).  GeoTIFF tags are also extracted by "kdu_compress" and used
  to create a GeoJP2 box if appropriate.  GeoJP2 boxes may be unpacked (if
  required) and/or written back to GeoTIFF files by "kdu_expand".
     The native TIFF managing code is small and does not explicitly manipulate
  imagery; it only manages TIFF directories -- see "kdu_tiff.h" or lookup
  the hyperdoc documentation for `kdu_tiffdir'.  Kakadu is actually quite
  agnostic about the GeoJP2 format, but since a GeoJP2 box is just a UUID
  box which reads like a TIFF file, any such box can be supplied directly
  to `kdu_tiffdir::opendir', giving you read/write/modify access to its
  embedded tags -- double-precision world coordinates and such.
     The demo code in "kdu_compress" and "kdu_expand" only reads and writes
  uncompressed TIFF files -- although it handles arbitrary bit-depths from 1
  to 32 bits/sample and both signed and unsigned sample formats.  If you
  need to read compressed TIFF files, this is still possible by defining
  `KDU_INCLUDE_TIFF' and linking against the public domain LIBTIFF library.
  If you do this, the services of LIBTIFF are used only for decompressing
  source TIFF files (and then only when the file indicates that it uses a
  compressed sample format) -- all other interaction with the TIFF directory
  is managed natively.

* "kdu_hyperdoc" now generates a much more extensive set of Java
  interfaces to Kakadu functions.  In particular, most combinations of
  default arguments in the C++ interface functions are now converted
  into distinct Java bindings, for programming convenience.  Also,
  functions which accept or return opaque pointers (i.e., pointers to
  objects whose definitions are not publically visible) are now
  mapped to Java bindings, with the pointers represented as Java "long"
  arguments.  There are some important high level interface objects,
  such as "jpx_frame_expander" which manipulate such opaque pointers and
  so were not previously available in Java.

* Quite a number of minor bugs have been fixed (should include all bugs
  reported to date).  These include:
  -- Fixed a minor bug in the parsing of open-ended ranges (e.g., unbounded
     codestream indices) in JPIP communications.
  -- Fixed a minor syntax error in the parsing of the "metadata-only"
     qualifier for JPIP requests -- this qualifier consists of a "!!" found at
     the end of "metareq" request field, where kakadu previously expected
     ",!!".
  -- Made some minor improvements to the distortion-rate slope prediction
     algorithm at the heart of the EBCOT block encoding machinery in the
     core system; this algorithm is used to prematurely truncate the
     encoding process where a minimum slope or an overall maximum length
     threshold has been provided to the codestream management machinery.
     The previous implementation very occasionally truncate code-blocks
     much too early.  The new implementation is much less likely to do
     this, but sacrifices nothing in speed.
  -- Fixed a minor bug in the "kdu_hyperdoc" utility which caused it to
     append "[SYNOPSIS]" style comments found against protected or private
     class members to the descriptions of preceding public class members.
  -- Fixed a minor race condition in the "kdu_client" object, which could
     manifest itself when both server and client try to close a connection
     in very close proximity.
  -- Fixed a minor bug in the order in which Kakadu's JPIP server component
     processed cache model manipulation statements from the client.  This
     caused model addition statements to be discarded if they were
     delivered on a first request which accessed a particular codestream.
  -- Fixed a minor oversight in "kdu_compress" which prevented
     fragmented compression from working correctly with JPX files -- the
     problem was that the contiguous codestream box was not being placed
     into the "rubber length" mode when writing JPX files (any application
     can easily select this mode), so that extra fragments which appended
     this box were not included in the indicated box length.
  -- Fixed a minor bug accidentally introduced into the Altivec (PowerPC)
     speedup code in version 5.0.  The bug involved 64-bit pointers being
     cast to 32-bit integers for parity checking, which fails to achieve
     the desired result in the big-endian PowerPC architecture.  The bug
     only manifested itself near the left edge of some regions when using
     Kakadu's region-based decompression features.

Changes from version 4.5.2 to 5.0
---------------------------------
* Four new dead-easy demo code fragments implemented inside the
  "kdu_render" demo app, to get you up and running as quickly as
  possible.  The most sophisticated of these (embodied in function
  `render_demo_4') renders any raw codestream, JP2 file, JPX file,
  JPX animation frame or MJ2 file to a memory buffer, performing
  colour space conversions as required, inverting multi-component
  transforms as required, etc., etc. -- it is less than 50 lines long,
  but if you like one-liners, just consider a call to `render_demo_4'
  as your one line solution.
* Windows builders note that some applications compiled against the
  core system DLL may need to define the symbol "CORESYS_IMPORTS".  It
  is good practice to do this universally in all your applications which
  import the Kakadu core system DLL.
* Windows builders using Microsoft VC6 need to ensure that they have the
  processor pack installed (see "Compilation_Instructions.txt").
* Added full support for Part-2 arbitrary transform kernels
* Added full support for Part-2 arbitrary decomposition styles, including
  wavelet packet decomposition structures and unbalanced sub-sampling
  structures in which successive resolution levels might have identical
  horizontal dimensions or identical vertical dimensions.
* Added full support for Part-2 multi-component transforms, including
  all possible transform block types (reversible/irreversible decorrelation,
  dependency and DWT transform blocks with arbitrary kernels).  Also
  ensured that all other elements from rendering to interactive distribution
  with JPIP work correctly when multi-component transforms are employed.
  Note that JPIP syntax only allows requests for multi-transformed components
  to be signalled when they are wrapped as JPX compositing layers -- use
  the `-jpx_layers' argument to "kdu_compress", together with "-jp2_space sLUM"
  if you want to create one compositing layer for each multi-transformed
  image component (say in a medical volume) and then interact with it using
  JPIP.  See "Usage_Examples.txt" for more ideas or read the updated overview
  document, "kakadu.pdf" -- see especially the new Section 4 in this updated
  document.
* Modified codestream parameter sub-system to use correct Part-2 codestream
  syntax for codestreams which use the alternate code-block alignment
  required for compressed-domain rotation and flipping (as performed by
  "kdu_transcode" for example) -- previous versions included this only as
  an experimental feature, without correct marker segment syntax.
* Improvements to platform-dependent speedups allow throughput increases of
  around 20% at low to moderate bit-rates on Pentium-4 platforms, while the
  Altivec speedups for the Power PC now function correctly under all
  conditions, including regoin-of-interest decompression -- previous versions
  had an alignment-induced problem under some conditions, but the new version
  aligns all critical accesses on 16-byte boundaries, on all platforms.
  This will also improve cache performance when multi-processor speedups are
  introduced in a future release.
* Rendering speed at low to moderate bit-rates via `kdu_region_compositor' or
  `kdu_region_decompressor' has increased by 40% to 50% on Pentium 4 platforms,
  due to the selective inclusion of SSE speedups in the decompressed data
  processing path -- smaller speedups should already be observed on other
  platforms, but it should be very easy indeed for interested persons to port
  the small Pentium 4 speedup routines in "msvc_region_decompressor_local.h"
  and "msvc_region_compositor_local.h" to other platforms (e.g. the Power PC),
  following the general approach taken in the core system (see gcc
  platform-specifics in "coresys/transforms" for examples).
* Fixed a bug in the fragmented codestream compression feature, whereby
  the quality layer target lengths supplied to `kdu_codestream::flush'
  were incorrectly scaled when multiple fragments were used.  Some
  licensees chose to avoid this problem by using slope thresholds to control
  quality layer sizes, which is probably the best strategy for fragmented
  codestream compression.  Others pre-scaled their target layer lengths to
  compensate for the internal bug.  This latter group will now need to
  remove this pre-scaling, since the internal problem has been corrected.
* Introduced a heuristic adjustment to the efficient rate control strategy
  implemented by the "kdu_v_compress" demo application so as to prevent
  occasional hicups reported by others.
* Added the capability for "kdu_compress" to read raw files in little-endian
  word order (just use files whose suffix is ".rawl" rather than ".raw").
  Also added the ability to read raw files which contain multiple
  concatenated image components (for simplicity when compressing image
  volumes).
* Added a `-codestream_components' option to "kdu_expand", so that users
  can specify whether they want to produce only the codestream components
  (these are the components produced after block decoding and inverse
  wavelet transformation) or the complete output components (these are
  the components produced after any specified colour or multi-component
  transformations have been performed).  If you do not specify this option,
  you will get output components.
* Incorporated 3'rd party fixes to the "kdu_hyperdoc" utility to generate
  Java (JNI) interfaces which avoid a rare race condition previously
  encountered in multi-threaded Java apps.

Some existing applications may require the following changes in order
to fully support Part-2 codestreams.
1) In order to ensure correct generation of JPX image header boxes
   when Part-2 codestream features may be used, your application must now
   be sure to invoke the `jp2_dimensions::finalize_compatibility'
   function, after finalizing all `kdu_params' objects and before invoking
   `jpx_target::write_headers'.  This is a new requirement.  Previously, it
   was sufficient to just call `jp2_dimensions::init' any time after
   the `siz_params' data was available.  While not strictly necessary, it
   is a good idea also to call `jp2_dimensions::finalize_compatibility'
   right before generating a regular JP2 file's header (via
   `jp2_target::write_header') so that the logic can verify that the
   codestream being embedded in the JP2 file is indeed a Part-1 codestream,
   since Part-2 codestreams must be embedded in JPX, rather than JP2 files.
2) If your application uses `jp2_dimensions::copy' to copy imagery from
   an existing JPX file to a new one, you should ideally first
   invoke the `jp2_dimensions::finalize_compatibility' function on the
   source object.  To facilitate this process, the
   `jpx_codestream_source::access_dimensions' function now takes an optional
   `finalize_compatibility' argument, which you should set to true when
   accessing the source `jp2_dimensions' interface which you intend to supply
   to `jp2_dimensions::copy'.  Except where such copying is required, it
   is more efficient to leave the `finalize_compatibility' argument equal to
   its default value of false.
3) If your application needs to directly access codestream subbands, you
   should use the new `kdu_resolution::get_valid_band_indices' function
   to obtain the range of subband indices which are valid for any given
   resolution level.  In previous versions, the application could assume
   that resolution level 0 had only the subband with index `LL_BAND' while
   all other resolution levels had subbands with indices `HL_BAND',
   `LH_BAND' and `HH_BAND'.  This is still true for Part-1 codestreams, but
   you need to be more careful with Part-2 codestreams.  Very few applications
   should need to directly access subbands.  The one notable exception is
   transcoding applications, which typically copy/transcode the relevant
   code-blocks one-by-one, accessing them via their containing subband
   interfaces.
4) If you want your application to work correctly on images which have
   been compressed using JPEG2000 Part-2 multi-component transforms, some
   minor changes may be required as follows.  I believe that these represent
   everything you need to consider.  Also, none of these changes
   are required if you don't need to decompress images which use Part-2
   multi-component transforms, and many applications might not require any
   changes at all.
   a) If you are using `kdu_stripe_decompressor', `kdu_region_decompressor'
      or `kdu_region_compositor' you don't need to do anything differently;
      however, if you are creating `kdu_pull_ifc' derived objects
      (`kdu_decoder' or `kdu_synthesis') directly to construct decompression
      engines, you will only end up decompressing the raw codestream
      components.  To migrate to a decompression engine which also correctly
      inverts the multi-component transform is actually very easy.
      Essentially, you just need to replace all of your independent
      component processing engines with a single `kdu_multi_synthesis'
      object, which does the whole thing -- then you no longer need to look
      out for whether a YCC colour transform needs inverting either.  If in
      doubt, take a look at how the compression engine creation and usage
      code has been changed in the "kdu_expand" demo app.
   b) For completely general decompression, you need to bear in mind that
      there can be a difference between output image components (produced
      after any multi-component transform is inverted) and codestream
      components (the ones you may have previously been associating directly
      with decompression engines).  The difference may involve differences
      in order (and hence possibly dimensions) and number (there can be
      more or less multi-component transform output components than raw
      codestream components, in the general case).  For backward compatibility,
      Kakadu's interface functions which return information related to
      components or indexed by component indices actually return information
      about the raw codestream components by default.  However, all of these
      functions now contain an optional final argument (`want_output_comps')
      which you should set to true if your application wants information
      about final output components (more likely than not this is the case).
   c) If your application directly invokes
      `kdu_codestream::apply_input_restrictions' you need to know that there
      are now two versions of this function.  If you call the original
      version in the way you did before, it will make multi-component
      transforms appear not to be present (this is required for backward
      compatibility with applications which might have been directly
      creating low level decompression engines).  To avoid this, set the
      optional final `component_access_mode' argument to
      `KDU_WANT_OUTPUT_COMPONENTS', or consider using the second, much
      more flexible version of the function which now allows you to
      restrict your region of interest to any arbitrary set of image
      components, optionally including permutations.

Changes from version 4.5.1 to 4.5.2
-----------------------------------
This is just a bug fix version.
* Fixed a minor bug in "kdu_transcode" which arises when extracting
individual image components.
* Fixed a minor bug in `kd_message_block::peek_block' which caused
problems when the Kakadu client is communicating with a server via
a proxy which rechunks the HTTP response.
* Fixed a couple of accidentally introduced bugs in `kdu_region_compositor'
and the way it is used by "kdu_show", which caused the objects to be
used in a manner which was much less responsive to interactive control
than it had been in versions prior to v4_5.
* Made a minor change to `CKdu_showApp::save_as_jpx' to ensure that
large files could be re-saved without unnecessary intermediate buffering
of the embedded codestream box.

Changes from version 4.5 to 4.5.1
---------------------------------
NB: This version exists only to correct a few of minor issues
discovered immediately after the release of Version 4.5.

* Minor bug fix in `kdu_region_compositor' to avoid a problem
which can cause the number of available rendering resolutions to
be reduced by 1 (bug was accidentally introduced in v4.4 with
the move to arbitrary rational composition scaling factors).
* Minor correction to `kdu_region_decompressor' to ensure that
high bit-depth imagery which requires colour conversion will
be handled correctly, even if not with maximum accuracy.
* Added the `JPX_SF_sYCC' feature flag to "jpx.h" in accordance
with a recent ammendment to the IS 15444-2, which describes
the JPX file format.  This should not impact any existing
applications.

Changes from version 4.4 to 4.5
-------------------------------
* Introduced all the support required to internationalize and/or
  customize all error/warning messages produced by the Kakadu
  system or derivative applications.  This is done in an almost
  seamless manner, which remains backward compatible with previous
  uses of the "kdu_error" and "kdu_warning" services and which
  is entirely platform independent.  Original error/warning text
  remains in the source files where it is easiest to follow and
  edit.  However, new constructors are provided for "kdu_error"
  and "kdu_warning" which allow text to be registered with unique
  identifiers.  This is done using macros, so that there is
  very little editing of the existing error/warning calls.  The
  same macros are used by a new tool, "kdu_text_extractor", to
  collect all the registerable text into separate language files.
  You can create as many versions of these as you like, translating
  text into new languages (and even using unicode for languages
  with large alphabets).  To use any of these external language
  files, you compile the original code (core system, applications,
  etc.) with the `KDU_CUSTOM_TEXT' macro defined.  You then simply
  include the language files of interest into your end application
  and the translation process is complete.  If you like, you can
  construct separate language-specific DLL's (Windows) or
  shared librares (Unix) containing the language files.  Your
  application then just needs to load the language DLL or shared
  library of interest at run time.
    By default, `KDU_CUSTOM_TEXT' is not defined, and everything
  behaves exactly as it did in previous versions of Kakadu.  In
  this case, there is no need (indeed no point) including the
  language files.
    For more information, see the "Compilation_Instructions.txt" file.
* Extended `kdu_region_compositor' to support Motion JPEG2000 data sources,
  in addition to JP2/JPX files and raw codestreams.  The new support includes
  the ability to composit and individually reorient tracks in accordance
  with the movie specifications found in the Motion JPEG2000 source.  Some
  of these new features are demonstrated by new features in "kdu_show".
* Extended "kdu_show" to handle Motion JPEG2000 files, in addition to its
  current input formats.  Also added playback control which operates in the
  same way for Motion JPEG2000 movies and JPX animations.
* Updated the `mj2_source' and `mj2_target' objects to bring them into
  compliance with aspects of the Motion JPEG2000 standard which were
  changed/clarified by a corrigendum.  The interface functions offered by
  `mj2_source' now offer the support required for handling data sources
  which are fed by an asynchronous dynamic cache (for JPIP browsing
  applications); however, the internal implementation does not yet support
  caching sources.  This can wait until we have a full JPIP implementation
  for video browsing.
* Extended the "kdu_merge" demo application, to support writing of both
  JPX and MJ2 files, based on input from one or more JP2/JPX/MJ2 files.
  The new features allow you to create MJ2 files from a sequence of JP2
  files, from the compositing layers in a sequence of JPX files, and/or
  from a sequence of existing MJ2 tracks.  You can write multi-track
  MJ2 files and you can even merge fields together to form interlaced
  MJ2 tracks -- however, note that "kdu_show" will currently only play
  the first field (correctly scaled) in each frame of an interlaced track.
     These services, while simple, close an important hole in
  the demo applications relating to Motion JPEG2000.  Many people had
  difficulty using the "kdu_v_compress" and "kdu_v_expand" tools in the
  past because they were not sure how to create "vix" files.  Now you
  can create MJ2 files from JP2 files and use them to write sample VIX
  files to play around with -- VIX is a mindbogglingly simple format though.
* Fixed a minor bug in `kdu_region_decompressor' which could affect the
  image produced when using rational expansion factors.  Also, replaced
  nearest neighbour interpolation with an efficient bilinear interpolation
  strategy for improved visual performance when non-integer rational
  expansion factors are used.
* Fixed a minor bug which was introduced into `kdu_region_compositor' in
  version 4.4 -- the bug could generate an assertion failure when zooming
  into rotated compositions.
* Fixed two minor bugs in the core system which affect transcoding.  You
  can now use incremental codestream output with `kdu_codestream::trans_out'
  for transcoding applications (lots of memory saving potential) -- a simple
  demonstration of this is included with the "kdu_transcode" application.
* Fixed a minor bug in the core system which has always been in Kakadu --
  it had the potential to cause an assertion failure (or memory overwrite)
  when re-opening a precinct with an increased quality layer threshold,
  but only in persistent mode where the codestream contains no PLT
  information, and then only with certain uncommon combinations of
  parse/read sequences.
* Removed the call to `kdu_error' within `kd_encoder::~kd_encoder' if not
  all lines were pushed into the compression engine.  This allows you
  to abort compression processing without errors or memory leaks.

Changes from version 4.3 to 4.4
-------------------------------
* Support for arbitrary scaling factors has been introduced into
  `kdu_region_decompressor' and `kdu_region_compositor'.  This means
  that you are no longer restricted to integer expansion factors and
  power-of-2 decimation.  It also means that correct interpolation
  can be applied prior to colour transformation or alpha blending, in
  the case of uncommon chrominance subsampling arrangements or low
  resolution alpha specification.  Most importantly, however, it means
  that `kdu_region_compositor' can correctly composit multiple
  codestreams onto a JPX compositing surface, taking into account their
  respective scaling requirements to produce correct alignment under
  all circumstances.
* Added capabilities to derive progress information in JPIP
  interactive browsing applications, based on the number of quality
  layers which are available at any given time for the codestream
  precincts required to reconstruct a region of interest.  This support
  is introduced by the low level functions `kdu_resolution::get_precinct_area'
  and `kdu_resolution::get_precinct_packets', and the much higher level
  function, `kdu_region_compositor::get_codestream_packets'.  The
  new functionality is explicitly demonstrated by the provision of a
  progress status indicator for "kdu_show", which shows up when the status
  bar is toggled into the download statistics mode during JPIP remote
  image browsing.
* Implemented all the machinery required to create and handle fragmented
  and externally referenced codestreams in a JPX file.  This is demonstrated
  by the provision of a "-links" argument to "kdu_merge" which can be
  used to create a JPX file whose codestreams reside in other files (e.g.
  a photo album whose contents reside elsewhere).  All
  demonstration applications correctly handle such JPX files, including
  "kdu_show" and "kdu_server".  When JPX files with external links are
  served using JPIP, streaming equivalents are used to make them appear
  local.
* Moved the MMX optimizations for Linux builds from assembler source files
  (*.s) to gcc inline assembly code.  This should have no impact on
  performance, but avoids the various problems which have been encountered
  in the past when compiling with different versions of GAS and GCC.
  Compilation for Linux should now be a hassle-free process.
* Added "-log" and "-wd" arguments to "kdu_server", following the suggestion
  of Michael Owen, so that it can be invoked as a registered Windows service.
* Some minor adjustments have been made to the definitions in
  "kdu_elementary.h" to facilitate compilation under 64-bit Solaris
  environments; also, revised makefiles for the sunpro compiler have
  been contributed by Margaret Lepley.
* Fixed a number of bugs accidentally introduced by the more extensive
  changes in v4.3.  Notable among these is a bug which manifests itself
  if `kdu_codestream::augment_buffering' is used.
* Fixed a number of bugs connected with more exotic colour rendering.
* No known bugs remain.

Changes from version 4.2.1 to version 4.3
-----------------------------------------
* Significant improvements in dynamic memory management for tiled images
  -- Previous versions of Kakadu provided for dynamic unloading of
     unused precincts, subject to the provision of appropriate pointer
     information in the code-stream.  This functionality has now been
     extended to the automatic unloading (and on-demand reloading) of unopen
     tiles from heavily tiled images.  While this is most efficient if the
     code-stream contains TLM marker segments to point to the tiles from
     its main header, the code-stream machinery now builds the TLM
     information on the fly if necessary, while parsing through seekable
     compressed data sources.
     + See `kdu_codestream::augment_cache_threshold' to understand how
       dynamic tile unloading/reloading has been integrated into the
       existing automatic memory management features -- by and large,
       applications should be able to get the benefits of these features
       without any implementation changes, remaining blissfully unaware
       of the numerous code-stream structures which could be presented
       to the internal machinery.
     + The new function `kdu_codestream::set_tile_unloading_threshold'
       gives you additional control over dynamic tile unloading, if you
       should require it.
     + The `kdu_codestream::get_compressed_state_memory' function has
       been slightly redefined to take advantage of more comprehensive
       memory accounting by Kakadu's code-stream memory management
       system.  In particular, the cost of tile, tile-component,
       resolution, subband and precinct address structures are now
       included along with the cost of precinct and code-block structures
       which were previously reported by this function.  By and large,
       applications which were using this function for gathering memory
       statistics, can continue to work as before, except that the statistics
       will be more comprehensive.

* Improved the efficiency with which finely tiled images are compressed
  and manipulated at very low resolutions, where there can be an enormous
  number of tiles.  This is achieved by maintaining a cache of "typical"
  tile objects within the code-stream management machinery, rather than
  instantiating a distinct object each time a tile is opened.  This
  functionality leverages off the changes first introduced in v4.2,
  wherein the coding parameters of finely tiled images are represented
  very efficiently wherever possible.

* The core codestream generation machinery can now generate TLM marker
  segments itself, rather than having to defer this process to a postprocessing
  phase using the "kdu_maketlm" program.

* The core codestream generation machinery can now compress tiled images
  in fragments, where each fragment is compressed independently and can
  consist of any subset of the tiles in the full codestream.  Fragments
  are automatically stitched together and a correct set of TLM marker
  segments can be generated for the full set of fragments, by rewriting
  selected segments of the main header.  These capabilities extend
  Kakadu's ability to compress large images at least into the tens of
  tera-bytes, and are demonstrated with the aid of the new `-frag'
  argument to the "kdu_compress" demo utility (see "Usage_Examples.txt").

* The `kdu_region_decompressor' object now offers even more flexibility
  in the way its `process' functions set the dynamic range and signed/unsigned
  attributes of the samples which they produce.  You can supply a
  `precision_bits' argument of 0 to get the `process' function of interest
  to derive its precision and signed/unsigned decisions from the original
  image sample attributes recorded in the codestream and/or file-format
  headers as appropriate.  The `kdu_channel_mapping' object's configuration
  state is expanded to handle this information and also allow
  application-specific overrides for each individual logical rendering
  channel.  These changes seem to address the concerns of a wide range
  of applications, as expressed on the Kakadu mail reflector.

* The JPX file format reader can now read JPX files which were mistakenly
  written without the reader requirements box by Algovision -- JPX
  files without any reader requirements box were technically illegal, but
  the reader requirements box is so poorly defined as to be next to
  useless to practical applications, so it is better not to require its
  existence, rather than fail to read existing non-compliant files.

* The JPIP client/server implementation now allow a TCP transport
  channel to be preserved between JPIP sessions, so you can start
  and close multiple JPIP sessions without having to close and
  re-open the TCP link (good for apps which need to make lots of
  JPIP requests without looking like they are trying a denial of
  service attack).  To do this, you must pass a non-default value
  for the `keep_transport_open' argument to `kdu_client::disconnect'.
  The operation should succeed in preserving the channel until the
  next call to `connect' so long as you are using the HTTP transport
  method and the connection was idle when you called `disconnect'.
  You can test the functionality out in "kdu_show" by explicitly
  clicking File->Disconnect, after you see the idle status appear in
  the statusbar.  The next connection attempt will try to use the
  same TCP channel; if the server has dropped the channel already
  (e.g., due to timing out on waiting for more communication on it --
   "kdu_server" uses a default timeout of 5 seconds), a new one
  will be opened.  Note, however, that in this mode there is no
  checking to see if the new channel belongs to the right server, so
  you may get an error message back if you are really trying to
  request a resource from a different server.  The application is
  supposed to avoid chaining requests which belong to different
  servers on the same TCP channel.

* Fixed a number of bugs, including
  -- one subtle bug in "kdu_server" (provided by a licensee), which could
     cause assertion failure under some circumstances when the same image
     is being accessed from multiple threads;
  -- several other rare bugs in the synchronization logic used for
     the producer-consumer threads in "kdu_server" -- these fixes may
     have resolved previously reported occasional server hangups;
  -- a number of inter-related bugs affecting complex colour conversion
     processes in the `jp2_colour_converter' object -- in particular, Lab
     colour space conversion now works correctly.
  -- a bug introduced accidentally in v4.2 which prevented RGN marker
     segments from being written out during code-stream generation.  This
     bug was responsible for problems encountered in the generation
     of images with explicit ROI features.


Changes from version 4.2 to version 4.2.1
-----------------------------------------
* This is a bug-fix release.  Below is a summary of some of the bugs fixed:
  -- A bug in the SPARC optimizations (VIS instruction set) which occurred
     when decompressing regions of interest.
  -- Various bugs in the "kdu_server" utility.  Most of these manifested
     themselves only when serving certain types of image files; however,
     I believe I have fixed a long standing bug which would cause the
     server to sometimes experience a memory fault after running for a
     month or so.
  -- A minor bug in the "kdu_show" application which manifested itself
     sometimes while closing and re-opening JPIP sessions.
  -- Various problems with the TIFF reading code originally contributed by
     a third party to the "kdu_compress" demo app -- substantially rewritten.
  -- Some bugs in the `kdu_stripe_decompressor' object and associated
     demo apps, which caused some types of data precision conversions to
     be performed incorrectly.

Changes from version 4.1 to version 4.2
---------------------------------------
* JPIP Client-Server extended to fully support JPX
  -- the client-server implementation has been updated to support the new
     "codestream-context" request field defined by JPIP.
  -- using codestream contexts and context translation, the client-server
     implementation now allows for highly efficient browsing of complex
     JPX images, containing multiple compositing layers, with different
     scale factors, cropping, and placement.  Also supportes efficient
     browsing of remote images which use multiple codestreams, potentially
     at different scales, to construct each image layer.
  -- the client-server implementation now fully supports the interactive
     transfer of metadata, including textual labels, XML, etc.  The
     server automatically figures out what metadata is relevant to any
     given requested image region.  Moreover, it is able to do this in
     the case of complex images, having multiple codestreams at different
     scales, cropping, and placement, each with its own codestream-registered
     region specific metadata.  The bare minimum amount of metadata is
     delivered to satisfy any particular request.  Moreover, all
     metadata-specific JPIP requests ("metareq" request field) are
     properly honoured and scoped.
* Dramatic speedups for heavily tiled images
  -- many thanks to Margaret Lepley for pinpointing the source of serious
     slow downs which have been observed when compressing/decompressing
     images with a very large number of tiles.  The problem was in the
     linear list searching performed in the codestream parameter sub-system
     ('kdu_params' and its derived objects).  This sub-system was actually
     the very first code implemented when Kakadu was originally developed.
     The internal implementation of "kdu_params" has been significantly
     changed to dramatically reduce access times (by up to 3 or 4 orders
     of magnitude) and memory consumption, while leaving the interface
     functions almost entirely unchanged.  No changes should be required
     by any existing application-level code, although some applications
     which use the binary methods to access parameters could stand to
     save memory now by supplying a value of "true" for the new "read_only"
     argument to `kdu_params::access_relation'.  See the interface
     documentation for this function if you wish to take advantage of this.
* SIMD platform-specific speedups to the wavelet and colour transform code
  are now provided for the UltraSparc (Sparc VIS instruction set) and the
  PowerPC G4 (Altivec vector processing instruction set), to complement
  the existing SIMD speedups provided for Intel processors (MMX instruction
  set).  Many thanks to Monroe Williams for contributing the Altivec
  code, while working on Linden Lab's "Second Life" project.
* To further demonstrate the sophisticated JPX features, the "kdu_merge"
  application has been extended to include:
  -- a "photo album builder" function, whereby an arbitrary collection of
     JPEG2000 compressed images can be combined into a single photo album,
     which can then be served as a monolithic entity for highly efficient
     interactive remote image browsing; and
  -- merging of metadata from all source images into the single output JPX
     image, after correctly adjusting and/or inserting cross-references
     between metadata items and image elements.
* The "kdu_show" application has been slightly updated to allow image-wide
  metadata to be displayed and edited interactively.  In the simplest
  instance, this allows the user to view labels which may be attached
  by a content provider to each compositing layer in an image.  In the case
  of photo albums built using the "kdu_merge" tool, each image in the
  album is a separate compositing layer, so the labels might be image
  descriptions.  Of course, these all get delivered correctly by the JPIP
  client-server machinery, during interactive remote image browsing.
* Removed the Intel SSE instruction, PEXTRW, from the code used to accelerate
  image blending in "kdu_region_compositor.cpp", so that plain-old MMX
  compilers and processors will be able to use the accelerations.  This
  results in a very slight slow-down in the accelerated compositing code.
  The blending accelerations have not yet been implemented for other
  SIMD architectures (e.g., Sparc VIS and Altivec), so these will just use
  the default platform-independent implementation.
* "kdu_compress" now supports reading of TIFF images, due to some
  code contributed by John Novak.  To enable TIFF reading, you need to
  define the macro, KDU_INCLUDE_TIFF; you need to obtain the "libtiff"
  package yourself and link against it, but this is very simple.

Changes from version 4.0.3 to version 4.1
-----------------------------------------
* All-new extensive support for JPX files, including the following:
  -- read and write all aspects of virtually all interesting JPX files,
     with only the following exceptions (these should both be included in
     the next release):
     * cross reference boxes and fragment tables not yet implemented
     * no explicit parsing or writing of desired reproduction boxes
  -- read and write multiple codestreams and multiple compositing layers
     * optimizes the use of default parameters where possible, during
       writing.
     * generates a comprehensive reader requirements box
     * maximizes potential for generated files to be JP2 compatible
  -- supports the extended colour descriptions offered by JPX, and manages
     multiple alternate colour descriptions.
  -- extended colour conversion capabilities:
     * allows conversion of the vast majority of JPX colour representations
       to sRGB; of the large number of enumerated JPX colour spaces, only
       CIEJab cannot currently be converted; of the embedded ICC profiles,
       only those involving 3D lookup tables cannot be converted.
     * application can control the trade-off between accuracy and speed
       for colour conversion.
     * application can request extended-gamut conversion
  -- supports all aspects of the JPX composition box, including composition,
     cropping, scaling, positioning and animation.
  -- extensive metadata management facilities
     * imposes a scale-space hierarchy on top of all spatially-sensitive
       metadata to facilitate efficient access.
     * uses scale-space structure to write metadata in a manner which is
       optimized for remote client-server delivery via JPIP.
  -- designed from the ground up to work with client-server systems
     * all aspects of the JPX interface allow for data to arrive
       asynchronously and out of order; interfaces to codestreams,
       compositing layers, etc, may become available incrementally and
       aspects of the data source are parsed only on demand.
     * capabilities proven and tested in the context of Kakadu's JPIP
       client-server architecture
* v4.1 provides a new very high level object, `kdu_region_compositor',
  which builds enormously on the capabilities of `kdu_region_decompressor'.
  This object provides the following new services:
  -- wraps up all the buffer management logic which was currently embedded
     in "kdu_show" within a single platform independent object.
  -- provides full support for compositing multiple images onto a single
     rendering surface, implementing the instructions represented by a
     JPX composition box; supports animation of complex image compositions.
  -- provides an efficient platform-independent alpha-based compositing
     engine.
  -- allows up to two codestreams to be used to build each compositing
     layer (one for colour and one for alpha), and any number of compositing
     layers to be combined on the rendering surface; all of this works
     together with interactive navigation, and dynamic delivery of image
     data from a remote JPIP server, if relevant.
  -- provides extensive integration of metadata (labels, xml, etc.) with
     the imagery, including overlays to symbolically represent the presence
     of spatially sensitive metadata; these are exploited and demonstrated
     by "kdu_show".
* many new features for the "kdu_show" demonstration viewer
  -- exploits all the new JPX features and the many features of the new
     `kdu_region_compositor' object.
  -- allows interactive stepping through animated image compositions,
     individual compositing layers, and even individual components or
     individual codestreams; these capabilities are also hyperlinked to
     the metashow tool.
  -- indicates the presence of spatially sensitive metadata via overlays
     (all implemented by `kdu_region_compositor'), including dynamically
     changing (blinking) overlays.
  -- dynamically overlays metadata text labels as the interactive user
     moves the mouse cursor (while control key is depressed).
  -- provides metadata editing and inspection facilities, with the ability
     to save images whose metadata has been edited; this provides an easy
     to use facility for marking up regions on an existing image.
* a new tool, "kdu_merge", allows complex JPX files to be generated by
  combining information from multiple JP2, JPX and/or MJ2 files
  -- can create composited and animated image sequences in a single JPX file.
  -- can build custom new JPX compositing layers by combining information
     from individual (or even multiple) codestreams in existing files with
     user-defined colour-space specifications.
* new JPX features reflected in various other demo applications, including
  "kdu_compress" and "kdu_expand".
* some minor fixes to the "kdu_server" application
  -- note, however, that kdu_server has yet to be upgraded to a JPX aware
     server.  One reason for this is that some changes to the JPIP standard
     are expected at the December 2003 WG1 meeting, particularly to meet
     the needs of serving JPX files properly.  While JPX files can be served,
     complex files (e.g., composited or animated images) will not
     automatically be served in a manner which meets the expectations of an
     interactive user.
* a memory growth problem has been fixed in the MJ2 file format writing logic.
* some fixes have been included for 64-bit addressing environments; these
  should fix some erratic behaviour during decompression of massive
  compressed images (4GBytes and up, compressed).
* slightly augmented the `kdu_region_decompressor::process' interface so
  as to allow the application to control whether or not the buffer row
  gaps are expressed in terms of pixels or samples -- the changes are
  transparent to existing users of the interface, but allow new applications
  to write directly to buffers with non-pixel-oriented alignment constraints
 (e.g., Windows bitmap buffers).

Changes from version 4.0.2 to version 4.0.3
-------------------------------------------
* Updated the JPIP implementation to make it current with the FCD produced
  out of the Strasbourg ISO/IEC JTC1/SC29/WG1 (JPEG) meeting.  There are,
  unfortunately, a couple of non-backward-compatible changes, so the client
  and server components of this version are not compatible with those of
  version 4.0.2.  The only significant respect in which the current
  implementation is not quite compliant with the JPIP specification is that
  the "mset" request field is not yet implemented -- there is no point in
  implementing this until I have a demo involving targets (e.g. video) with
  a large number of code-streams.  It is unlikely that the "mset" feature
  will be used for regular image browsing applications.  Preference and
  capability signalling by the client are also currently ignored, but in
  most respects Kakadu contains a very comprehensive implementation of the
  JPIP standard.
* Corrected an accidental change introduced to the behaviour of
  `kdu_codestream::flush' made in v3.4, in which the automatic layering
  heuristic introduced 1 layer per octave change in bit-rate -- the
  interface documentation states that the heuristic inserts 2 layers
  per octave (factor of 2) change in bit-rate where upper and lower
  bounds are not explicitly supplied by the application.  The documented
  policy has been restored.
* Included some changes provided by Michael Owen, to add an alternate
  interactive panning mechanism to "kdu_show".  Pressing the shift
  key together with the left mouse button, you can drag the view
  window around with the mouse in a natural way.
* Added a hex dump facility to the "metashow" tool in "kdu_show".
* Added arguments to "kdu_v_expand" to control the quality (number of
  layers), resolution (number of DWT levels) and spatial region associated
  with decompressed video frames.
* Added an optional array argument to the sample processing interfaces
  offered by the high level support objects, `kdu_stripe_compressor' and
  `kdu_stripe_decompressor' so that the application can explicitly control
  whether each of the image components uses a signed or an unsigned
  representation.
* Some minor bug fixes.

Changes from version 4.0.1 to version 4.0.2
-------------------------------------------
* Fixed a bug in the core system which gets excited under
  some configurations of tile opening and closing operations
  when running in the non-persistent mode.  This bug had always
  existed, but could not be uncovered by any of the demo
  applications prior to "kdu_buffered_decompress".
* Fixed a minor bug in the determination of whether or not
  a precinct is significant (in calls to `kdu_precinct::size_packets')
  which sometimes caused the Kakadu server to deliver certain image
  regions very late.  This bug was introduced by some new code to
  improve the efficiency with which small precincts are delivered
  by Kakadu's JPIP server.  Significance determination was not
  available prior to version 4.0.
* Added some new code to avoid needless reading of packets and
  tile parts which are known to be irrelevant to the application,
  where the image is tiled.  Kakadu has always been able to
  selectively parse the code-stream based on the available coding
  options, the application's needs and the availability of random
  access information, but these capabilities have principally been
  targeted toward untiled code-streams.  Since certain military
  applications prefer to use tiles, Kakadu is now much more careful
  about abandoning the parsing of tile data as soon as possible
  when operating in non-persistent mode (persistent mode provides
  no guarantees as to what elements of the code-stream may be
  required at a later stage, in which case efficient access is
  possible only if the code-stream is also endowed with sufficient
  pointer marker segments).

Changes from version 4.0 to version 4.0.1
-----------------------------------------
* Fixed a bug in 1-line of code in "kdu_params.cpp" which adversely
  affected any application which required transcoding services in
  which the original image used precincts with different dimensions
  in at least three different resolution levels.

Changes from version 3.4 to version 4.0
---------------------------------------
This is a MAJOR upgrade, introducing many new features, and providing
an implementation of the new JPIP standard for interactive imaging, which
is now becoming more stable, having reached CD (Committee Draft) status.
A summary of some of the key new features appears below, with the more
important changes listed last.
* All reported bugs have been fixed. For many of these, incremental workarounds
  have previously been published on the Kakadu public news forum, unless
  they were particularly esoteric.  Amongst the various bug fixes are the
  following:
  -- A bug was detected with the compression of tiled video frames using
     "kdu_v_compress".
  -- A bug was reported a long while ago with the construction of Java
     native interfaces.
  -- A bug was reported in connection with the use of certain image
     offsets (canvas coordinates) during compression -- bug was introduced
     accidentally while implementing the incremental code-stream flushing
     feature.
  -- While not exactly a bug, a number of users have reported occasional
     artefacts created while compressing images using Kakadu's block
     truncation prediction algorithm (not everyone is aware of the fact
     that this algorithm is enabled by default).  Although this algorithm
     is an optional speed enhancement, and not guaranteed to be 100% reliable,
     the new version contains a small code fix which dramatically improves
     its reliability with negligible impact on compression speed.
* The `kdu_codestream::set_max_bytes' function now contains an optional
  `simulate_parsing' argument which can be used to simulate the effects of
  parsing away any image components, layers or image resolutions which are
  rendered irrelevant by previous calls to `apply_input_restrictions', before
  applying a byte limit to the size of the resulting code-stream.  Otherwise,
  the byte limit simply truncates the compressed code-stream on input.
  This mode is useful for simulation work.  It may be enabled from the
  "kdu_compress" application, by specifying the `-simulate_parsing' command
  line option.
* "kdu_compress" now supports 32-bit BMP files, the definition of an
  explicit alpha channel (via `-jp2_alpha'), and inclusion of arbitrary
  additional meta-data in JP2 files (via `-jp2_box').
* The widely used `kdr_region_decompressor' object, originally designed as
  the platform independent component of the interactive rendering application,
  "kdu_show", has been given a major face lift.  It is now known as
  `kdu_region_decompressor' and found in the "apps/support".  The object
  now supports the decompression of additional (non-imagery) channels
  such as alpha channels, plus a wide range of buffer structures and data
  precisions to suit just about any region-based decompression application.
  The need to customize this object to meet specific application requirements
  should no longer exist.
* Two new high-level support objects, `kdu_stripe_compressor' and
  `kdu_stripe_decompressor' now wrap up almost all the steps required
  to build a memory-based compression application or a memory-based
  decompression application (although for interactive rendering, you will
  find `kdu_region_decompressor' more convenient than
  `kdu_stripe_decompressor').  They support a wide range of memory buffer
  architectures and data precisions to make application development much
  simpler than before.  Optional configuration arguments allow virtually
  all important Kakadu features to be accessed via these high level objects,
  with the image being compressed/decompressed from/to either a single
  buffer which holds the whole image or incrementally in stripes.
* The "simple_example_c" and "simple_example_d" introductory demo applications
  have been modified to use the `kdu_stripe_compressor' and
  `kdu_stripe_decompressor' objects mentioned above, making them much simpler
  again, while also more powerful.
* Two new example applications, "kdu_buffered_compress" and
  "kdu_buffered_expand" have been added to provide more comprehensive
  (though not complete) demonstration of the `kdu_stripe_compressor' and
  `kdu_stripe_decompressor' objects, and to bridge the gap between the
  very simple examples, "simple_example_c" and "simple_example_d", and
  the much more complex and comprehensive examples, "kdu_compress" and
  "kdu_expand".  The "kdu_buffered_expand" application can be significantly
  more computationally efficient than "kdu_expand" if you are working with
  a huge image which has been tiled.  This is because "kdu_expand" always
  tries to decompress one line at a time from the image, no matter how many
  tiles that line may span -- a reasonable approach in most circumstances,
  but certainly not the only way to use Kakadu.
* The "kdu_show" application has been augmented by the inclusion of a
  meta-data viewer (you can enable the "meta-show" facility by using the
  'm' accelerator, or from the view menu).  While the results might not
  be particularly interesting at present, this code demonstrates walking
  through an arbitrary JP2 family file, caching references to its elements
  and, MOST IMPORTANTLY, dynamically updating the meta-data display as content
  is dynamically transferred by a JPIP server.  The tool has been used to
  carefully test Kakadu's support for dynamic dissemination of both
  imagery and meta-data within the context of the new JPIP standard.  Without
  knowing much about JPIP, you might not be able to see much happening in
  the "meta-show" window yourself right now, but it is a key tool in the
  future road map for Kakadu.
* Kakadu's support for the JP2 file format has been completely revamped from
  the inside out.  If you are just creating and reading JP2 files, you will
  notice only one difference from previous versions.  You no longer open
  the JP2 source or target object directly.  Instead, you first open a
  `jp2_family_src' or `jp2_family_tgt' object, and then pass this to the
  `jp2_source::open' or `jp2_target::open' function.  Behind the scenes,
  however, the new architecture is very different:
  -- Firstly, Kakadu now provides a generic solution for all JP2 family
     file formats, by providing powerful `jp2_input_box' and `jp2_output_box'
     objects, which allow an application to navigate the contents of an
     arbitrary JP2 family file format.  Whenever a code-stream box is
     encountered, it may be passed directly to `kdu_codestream::create'
     to interact with or create the relevant imagery.  Multiple boxes may
     be open into a single file at any given time, allowing the application
     to interact with multiple code-streams simultaneously (amongst other
     things).
  -- Most important of all, the new `jp2_input_box' and `jp2_output_box'
     objects provide complete support for interacting with any object which
     is filling the role of a JPIP interactive image client.  They recognize
     and handle the special facilities provided by the new JPIP standard
     for hierarchically resequencing and streaming the content of an arbitrary
     JP2 family file, along with all of its meta-data.  These mechanisms
     are concealed from applications and file format parsers which are
     built on top of the `jp2_input_box' and `jp2_output_box' objects so
     that full support for efficient, user-sensitive, dynamic interactive
     services is automatically incorporated into any file format for which
     a parsing utility exists at the client side.  Currently, Kakadu provides
     the full set of parsing tools only for the JP2 file format, but the
     next version should bring JPX fully to life.
  -- One consequence of the above is that it is now a simple matter to
     add extra custom boxes to a JP2 file, and to parse these boxes out,
     opening any code-stream boxes as separate images.  These possibilities
     are demonstrated by the "kdu_compress" and "kdu_show" example
     applications, respectively.
* The `kdu_serve' object, representing the platform independent component
  on which Kakadu's image serving capabilities are built, has been thoroughly
  revamped to provide generic support for the various JP2 family file
  formats, interactive delivery of files with multiple code-streams
  (including video), and very much reduced memory consumption when
  deliverying huge images (those in the multi-Gbyte range).  It provides
  automatic as well as client-hinted scheduling of meta-data and image
  data in accordance with file-format and application-specific information
  provided by an auxiliary object, derived from the new `kdu_serve_target'
  interface.  Currently, only one `kdu_serve_target' object is implemented,
  which provides the intelligence necessary to digest raw code-streams and
  JP2 files, but all that is required to support custom JPIP service
  applications or new file formats is to derive an appropriate object
  from `kdu_serve_target'.  We expect to provide such an object for the
  JPX file format with the next release of Kakadu (v4.1).
* The platform dependent client and server components of Kakadu have
  been modified to conform with the syntax and conventions of the new
  JPIP standard (JPEG2000, Part 9), which is currently at CD (Committee
  Draft) status.  For more on JPIP and Kakadu's support for JPIP, consult
  the "jpip-links-and-info.html" file found in the "documentation" directory.
* The various documents, particularly the survey document "kakadu.pdf", have
  been updated to include introductory descriptions and overview diagrams
  for many of the new features offered by version 4.0.

Changes from version 3.3 to version 3.4
---------------------------------------
* The most significant change here is the introduction of incremental
  flushing of the code-stream during compression.  This allows platforms
  with only modest memory resources to compress truly massive images,
  provided you are careful to select the appropriate spatially progressive
  packet progression sequence.  Incremental flushing works with all modes
  of code-stream generation, although to use this feature efficiently you
  should review the extensive documentation which appears in the description
  of the `kdu_codestream::flush' function.
* The rate control policies have also been modified slightly to ensure that
  the most appropriate behaviour occurs when requested quality layer bit-rates
  span many orders of magnitude (e.g., "-rate -,0.001 Clayers=30").
* 64-bit data types and 64-bit file I/O are now supported when compiling
  under GCC, as well as Win32 environments where they have long been
  supported.  Be careful to set _FILE_OFFSET_BITS to 64 in the relevant
  make file, if you want this feature.  The Unix makefiles now also generate
  shared libraries.
* A significant problem has been fixed with the `kdu_server' application.
  The application had no memory or handle leaks per se, but some resources
  were not properly cleaned up until the server was in the process of being
  shut down due to failure to call the key clean-up function regularly.  The
  server also now responds more appropriately to HTTP received from browsers
  which are not JPIP capable.

Changes from version 3.2 to version 3.3
---------------------------------------
* The most significant change is the provision of new
  client and server implementations and support services.
  These are embodied principally through the platform
  independent objects, "kdu_cache2" and "kdu_serve2"
  and the derived platform dependent (WIN32) objects,
  "kdu_client2" and "kdu_server2".  The older versions
  of these objects (those without the "2" suffix),
  supporting the original JPIK protocol, are still shipped
  but are being deprecated.  The new client and server
  implementations support our proposal to the JPIP standard,
  which is compatible with the current working draft produced
  at the ISO/IEC JTC1/SC29/WG1 meeting in Boston, July 2002.
  For more information, the reader is referred initially to
  the supplied document, "jpip-kakadu.pdf", and then to the
  object interface descriptions, and the "Usage_Examples.txt"
  file.
* A couple of bugs in the Java interface building tools
  supplied in v3.2 have been fixed, particularly relevant to
  multi-threaded Java applications.  The fixes were
  contributed by one of our clients who is working extensively
  with Java.
* The "kdu_message" object has been given a new virtual (callback)
  member function, "start_message", which is particularly useful for
  synchronizing message delivery in multi-threaded environments.  This
  is used by the "kdu_show" application to ensure reliable message
  delivery from both the image processing thread and a network
  management thread.
* Minor changes have been made to the JP2 file interfaces to allow
  cleaner support for distributing JP2 files over networks.
* The `jp2_colour::init' functions which generate ICC profiles for
  custom luminance and RGB spaces have been modified to include all
  mandatory ICC tags, rather than just those which are described in
  connection with the JP2 file format.
* Core system services have been included for including user-defined
  comments into the code-stream and recovering them, if necessary.  The
  "kdu_show" application's "File->Properties" menu item now displays
  all text comments it finds in the code-stream.
* The `kdu_codestream::flush' function now supports the inclusion of
  a special COM (comment) marker segment, embedding information about
  the rate-distortion slopes and sizes of the code-stream's quality
  layers, for use in other applications.  "kdu_compress" uses this
  option by default, unless it is explicitly disabled with the `-no_info'
  flag.  The information recorded in the marker segment, if present,
  is used by "kdu_server" to optimize the delivery of data to
  interactive clients using Rate-Distortion performance criteria.
  A brief description of this new feature may be found toward the
  end of the "jpip-kakadu.pdf" document.
* The "kdu_hyperdoc" utility now shortens the generated HTML file names
  to fit within the 33 character limit imposed by some browsers deployed
  in the Apple Mac environment.  A command-line argument may be given
  to restore the original fully expanded file naming convention.
* Some minor bugs have been fixed, relating to parameters with very
  large values, on the order of 2^31 or greater.  This includes a fix
  for problems sometimes encountered when compressing images larger
  than 2GB in size.
* A bug has also been fixed in the processing of encode-time ROI's
  (max-shift method) having large up-shifts (Rshift values).  The
  bug caused incorrect calculation of the distortion contributions
  associated with blocks whose up-shift required a dynamic range
  exceeding 32 bits to fully accommodate both foreground and
  background regions.

Changes from version 3.1 to version 3.2
---------------------------------------
* The most significant new feature is the introduction
  of Java native interface bindings to virtually all
  exposed Kakadu objects and public functions.  The
  Java bindings are automatically generated by the
  "kdu_hyperdoc" utility, which also includes details
  of the Java interfaces with the extensive HTML
  documentation it builds.  See the file,
  "java-interfaces.pdf", for more on building and
  using the Java interfaces.
* Introduced some additional interface functions to
  a number of the existing Kakadu classes, where the
  existing functions do not bind well to Java (or
  perhaps other languages).  This ensures that key
  functionality is available from Java (and maybe later
  other languages) through at least some method.
* Completely eliminated all C++ stream I/O dependencies
  from the core Kakadu system, rationalizing the
  error and warning message handling mechanisms in a way
  which is amenable to foreign language bindings.  This
  has additional advantages, since support for the C++
  I/O stream libraries is shaky or missing on some
  target platforms used by our customers.
  DEVELOPERS SHOULD CAREFULLY REVIEW the new declarations
  of `kdu_customize_errors' and `kdu_customize_warnings'.
* Added an overloaded version of the `kdr_region_decompressor::process'
  function which writes decompressed region data to a compact
  array of pixels, each having a 32-bit representation.  This is
  more efficient for Java applications and also for native
  language implementations which do not keep their entire viewport
  in a single contiguous buffer.
* Included a Java demonstration application,
  "KduRender.java", which uses the Java native interfaces
  to decompress and render an image incrementally to a
  display.  This should work across multiple platforms,
  although we have not yet tested it on anything other
  than the Windows platform.
* Corrected a couple of minor, yet potentially
  dangerous bugs, accidentally introduced in the
  upgrade from version 3.0 to version 3.1.

Changes from version 3.0.8 to version 3.1
-----------------------------------------
* Added a "restart" option to the "kdu_codestream"
  interface, which enables the internal code-stream
  management machinery to be efficiently restarted,
  with particular application to video compression
  and decompression.
* Incorporated MMX optimizations into the compressor
  as well as the decompressor, and fixed a bug in
  the previous MMX implementation which prevented
  the irreversible 5/3 transform (actually a JPEG2000
  Part 2 feature, not Part 1) from behaving correctly.
* Incorporated substantial support for the Motion
  JPEG2000 (MJ2) file format, which complements and
  leverages off the existing JP2 file format support.
  The new implementation should be easier to extend
  to other members of the JP2-family, e.g., JPM or JPX.
* Three new demonstration applications are included:
  -- "kdu_render" shows how the platform independent
     "kdr_region_decompressor" object can be used
     to build JPEG2000 decompression into a whole
     host of applications at a very high level,
     with a minimal amount of effort.
  -- "kdu_v_compress" and "kdu_v_expand are video
     compression and decompression applications.
     There is no interframe compression involved here.
     Instead, only frame-by-frame compression is
     supported, using JPEG2000 code-streams for each
     frame (or field, for interlaced video).  The
     applications use either the Motion JPEG2000
     file format or a much simpler video file format
     which has been created purely for demonstration
     purposes.
* The customizable error and warning message services
  have changed slightly with the addition of a
  user-definable argument to the callback functions.
* Fixed a number of minor bugs

Changes from version 3.0.7 to version 3.0.8
-------------------------------------------
* There is only one significant change here: all
  of the documentation which was distributed throughout
  the publically includable header files is now automatically
  built into a fully integrated documentation system.  You
  should find that the new "kdu_hyperdoc" utility compiles this
  documentation as soon as it is built (using the Makefiles
  or the Visual C++ build environment).  If not, you can always
  build the documentation system manually by running the
  command found in "documentation/hyperdoc.bat" -- you should
  change into the "documentation" directory first.  The
  integrated documentation system is accessed through the
  "index.html" file in the "documentation" directory.
* The names of three member functions have been changed -- the
  previous names were particularly unhelpful.  Specifically,
  "kdu_params::describe_string" and "kdu_params::describe_strings"
  are now called "kdu_params::describe_attribute" and
  "kdu_params::describe_attributes", while
  "jp2_palette::get_num_components" is now called
  "jp2_palette::get_num_luts".
* Some minor bug fixes.

Changes from version 3.0.6 to version 3.0.7
-------------------------------------------
* Corrected a mis-interpretation of ambiguous text
  in the description of the JP2 palette box.  The
  entries appear in entry-major order, as opposed to
  component-major order which previous versions implemented.
  This is an important correction for compliance.
* Fixed a minor bug in "kdu_compress" which made it difficult
  to specify component-specific subband weights.
* Added code to read and write profile identifiers from the
  Rsiz code in the SIZ marker segment.  More importantly,
  Kakadu carefully checks and reports and violations of the
  restrictions associated with individual profiles.  Warning
  messages are generated rather than error messages, allowing
  non-complying code-streams to be read and, optionally,
  transcoded into streams which do comply the a stated
  profile restriction.
* Added support for reading BMP files with incomplete palettes.

Changes from version 3.0.5 to version 3.0.6
-------------------------------------------
* Changed the code for the sYCC space in the JP2
  colour box to 18 from 22 to reflect a recent
  ammendment to the standard.
* Fixed a bug in which marker codes in the range
  FF30 to FF3F (not defined by JPEG2000) were not
  ignored as they are supposed to be.
* Minor enhancements to the "kdu_show" GUI including
  migration to BitBlt to support WinCE users.
* Fixed (I hope) a bug in the "kdu-server" application
  which occasionally caused the server to go into an
  active polling state (rather than blocking on a
  pending condition), thereby chewing up masses of
  CPU time.

Changes from version 3.0.4 to version 3.0.5
-------------------------------------------
* Minor bug fixes in the error resilience section of the
core system, plus some minor efficiency improvements.

Changes from version 3.0.3 to version 3.0.4
-------------------------------------------
* Significant improvements in efficiency of the "kdu_server"
application.  A single machine should now be able to serve
up to perhaps 100 clients simultaneously.  The flow control
protocol has also been substantially improved so as to
more nearly achieve the network capacity while retaining
(improving) interactive responsiveness.

* The "kdu_show" application can now write out compressed
images as JP2 or J2C files, which is particularly useful
when browsing images using JPIK.  Other controls have been
added to access all of the available image components (not
just the first 4) and to directly input JPIK URL's from
the menu.

* A "focus box" capability has been added to the "kdu_show"
application, which allows the user to specify a region of
interest, independently of the view port established by the
window dimensions, scroll and zoom settings.  When used,
the focus box provides the focus for zoom operations and
the region of interest for JPIK remote image browsing
sessions.  When connected to a JPIK server, the size of
the focus box may be limited to maximum dimensions specified
by the server, enabling it to bound the memory and disk
access resources required to serve large regions of interest
to clients.

* A number of minor bugs have been fixed: one which prevented
the "kdu_server" application from behaving properly with
images having certain coding parameters; and one very subtle
bug in the core system which manifested itself on very rare
occasions.

Changes from version 3.0.2 to version 3.0.3
-------------------------------------------
* Minor changes in the "kdu_server" and "kdu_show" applications
  to fix bugs and simplify the description of the JPIK protocol.
* Fixed a bug in the core system which appeared when decompressing
  images containing PLT marker segments and certain tile-part
  organizations.
* Implemented the second form of the "jp2_target::open" function,
  which was accidentally omitted.
* Fixed a minor memory error reported by one client using purify.

Changes from version 3.0.1 to version 3.0.2
-------------------------------------------
* Minor changes in the "kdu_server" and "kdu_show" applications to
  fix a problem which was causing a lot of firewalls to reject the
  traffic.
* Delegation and remote administration added to the capabilities of
  the "kdu_server" application.  It is now possible to review status,
  upload files to and/or shutdown the "kdu_server" application
  remotely under password protection.  It is also possible to serve
  files from a cluster of networked machines with a single public
  IP address.

Changes from version 3.0 to version 3.0.1
-----------------------------------------
* Minor changes in "kdu_server" and "kdu_show" to support a variant
  of the JPIK protocol which replaces the mixed TCP/UDP transport with
  a TCP-only transport.  Opening a URL with the suffix, ":TCP", using
  "kdu_show" will request the TCP-only transport for ongoing
  communications with the server.  The server can support multiple
  clients, where some might use UDP and others not.

Changes from version 2.2.3 to version 3.0
-----------------------------------------
Summary:
        This is a MAJOR upgrade, introducing many new features and
     achieving most of what the Kakadu architecture was originally
     designed for.  The most major new features may be summarized as:
     1) Lots of support for random access into very large compressed files;
     2) The introduction of "interchange" code-streams, with direct access
        to JPEG2000 packet data.
     3) A comprehensive demonstration of Kakadu's support for interactive
        client-server applications.

     As in previous releases, the principle form of documentation is the
     descriptions of public object member functions (interface functions)
     appearing in the relevant public header files.  These have been
     carefully organized for you.  The "kakadu.pdf" document has only
     limited information on the new advanced features introduced with
     version 3.0.  Additional typeset documentation for advanced
     capabilities will be forthcoming in the not too distant future.

Assorted Details:
* Kakadu now employs a customizable variable length data type, "kdu_long"
  (64 bits on Win32 and 64-bit Unix architectures at least) for all
  quantities which may be proportional to image area (as opposed to
  image dimensions), including compressed data size, file seek addresses
  and so forth.
* The core "kdu_codestream" object is able to exploit various pointer
  marker segments which may (optionally) be present to gain random access
  into a JPEG2000 code-stream.  In particular, TLM (Tile-Part Length Main)
  and PLT (Packet Length Tile-Part) marker segments are no longer discarded
  by the code-stream parser.  The compressor can generate PLT marker
  segments (or include them during transcoding with "kdu_transcode");
  however, generation of TLM marker segments cannot be accomplished in a
  single pass operation.  For this reason, a separate utility, "kdu_maketlm",
  is provided to introduce TLM marker segments (if you do not intend to tile
  the image, PLT marker segments are much more useful for random access than
  TLM marker segments, although both may be included).
     The conditions under which the information in PLT marker segments can
  be efficiently utilized are somewhat complex.  The compressed data source
  must advertise the KDU_SOURCE_CAP_SEEKABLE capability (as described in
  "kdu_compressed.h") and all packets of each precinct must appear
  contiguously within the code-stream (the default LRCP packet sequence
  generated by the "kdu_compress" application does not have this property;
  use RPCL, PCRL or CPRL).  The system will generally warn you when PLT
  information is present and ought to be usable apart from the selection of
  an adverse organization for the code-stream.
     When PLT marker segments are available and are able to be utilized,
  a "kdu_codestream" object with the persistent mode set (see
  "kdu_codestream::set_persistent") will unload compressed data from
  memory as soon as possible, since the PLT information may be used to
  reload it on demand.  This permits access to massive files with relatively
  little memory consumption.  Moreover, the system supports internal caching
  to minimize the frequency with which data must be reloaded from disk or
  reparsed from raw code-stream packets. The cache size can be directly
  controlled using "kdu_codestream::augment_cache".
* The "kdu_codestream" object can interact directly with a caching
  compressed data source (one which advertises the KDU_SOURCE_CAP_CACHED
  capability), recovering JPEG2000 packet data directly from such a source
  in any order (partially or in full) as needed.
* A "kdu_cache" object is provided, derived from
  "kdu_compressed_source" which is able to efficiently cache compressed
  data which arrives in any order whatsoever from an external source.
* A "kdu_client" object is provided, derived from "kdu_cache", which
  implements all the functionality required for the client side of an
  interactive client-server application.  Unlike the platform independent
  "kdu_cache" implementation, the "kdu_client" layer is multi-threaded
  and hence platform specific (it is currently implemented only for the
  WIN32 platform, although ports to POSIX threads would not be difficult).
* The "kdu_show" application has been augmented with a richer user
  interface and is able to utilize the "kdu_client" object as one of
  its various "kdu_compressed_source" derived input sources.  In this mode
  the "kdu_show" application becomes a fully fledged JPEG2000 image browsing
  application.  See the "Usage_Examples.txt" file for examples of this.
* The "kdu_codestream::create" function now comes in three different
  forms, rather than just two.  You can create the object for "input"
  (i.e., decompression), for "output" (i.e., compression or transcoding), or
  for "interchange" (this is the new one).  A "kdu_codestream" object opened
  for "interchange" has neither a compressed data source nor a compressed
  data target.  You write code-block data to such an object just like an
  output object, but rather than generating a compressed data stream, you
  can assemble and retrieve JPEG2000 compressed packets directly from the
  object, in any order whatsoever.  Various tools are provided to support
  rate control funcionalities which you might find useful.  Interchange
  objects are provided primarily for use with server applications which
  can then ship custom-built packets to a remote client, which uses them to
  augment a a caching compressed data source.
* A "kdu_serve" object is provided which implements the base functionality
  required to serve compressed data packets to a remote "kdu_cache" object.
  Both "kdu_serve" and "kdu_cache" have platform independent implementations.
* A "kdu_server" application is provided to demonstrate client-server
  capabilities.  Building upon the functionality offered by "kdu_serve",
  it talks directly with the "kdu_client" object activated when the
  "kdu_show" application is started with a network address having
  the protocol prefix, "jpik:".  The server is multi-threaded and hence
  not platform independent (the current WIN32 implementation could be
  ported to POSIX threads without too much difficulty).  The server can
  support multiple clients and multiple sources simultaneously.  It
  can share a single compressed data source across multiple clients
  and is able to take full advantage of the presence of PLT and PLM
  marker segments in the source files to avoid loading any more of the
  source file(s) than is necessary.  The server interacts with the
  client to deliver only the information which is relevant to the
  client's current region, resolution and components of interest.
  Moreover, the server automatically performs in-place transcoding of
  source files to allow the data to be transported using the smallest
  spatial precincts (finest spatial granularity) compatible with the
  code-block dimensions used during compression.  This enables any
  JPEG2000 source file whatsoever to be served up to a client in a
  spatially sensitive manner.
* The "jp2_source" and "jp2_target" objects have been enhanced to
  enable JP2 data to be sourced from or delivered to any
  "kdu_compressed_source" or "kdu_compressed_target" derived objects.
  This immediately allows JP2 functionality to be incorporated into
  memory based compressed data streams, or even caching compressed
  data sources such as "kdu_cache".  This, in turn, allows the
  client-server capabilities described above to work with JP2
  files, as well as raw JPEG2000 code-streams.  For more info,
  you might like to initially consult the comments appearing
  in "simple_example_c.cpp" and "simple_example_d.cpp" and then
  look at "jp2_source::open" and "jp2_target::open".
* The compressor now supports much richer tile-part generation
  capabilities.  Even if the image is not tiled, the single image
  tile can still be split into multiple tile-parts based upon
  resolution, component or layer indices, as controlled by the
  new "ORGtparts" compression attribute (see usage statement for
  "kdu_compress").
* A number of minor bugs and irregularities have been fixed.

Changes from version 2.2.2 to version 2.2.3
-------------------------------------------
* Extremely minor changes to avoid mixed use of formatted and
  unformatted calls to "ostream" objects.  These appear to
  excite a bug in GCC version 3.0.  The only file affected is
  "params.cpp" in "coresys/parameters".

Changes from version 2.2.1 to version 2.2.2
-------------------------------------------
  Note: none of these have any impact whatsoever on executable code
  or DLL's.
* Renamed the "core" directory as "coresys".  A trivial change
  and my appologies for those whom this might adversely affect.
  However, the use of the name "core" was causing some people
  difficulties, since it is identical to the name of a Unix core
  dump file.
* Made the Linux MMX-optimized functions "extern C" instead of
  "extern", so as to avoid problems caused by different name
  mangling conventions between various versions of gcc.
* Eliminated multi-line comments from assembler files so as to
  avoid problems created by earlier versions of the gnu assembler.

Changes from version 2.2 to version 2.2.1
-----------------------------------------
* Replaced the C++ I/O routines used for image and compressed data
  transfers with ANSI C I/O functions.  This was motivated by the
  fact that the new-style ANSI C++ I/O package is unbelievably slow,
  at least as implemented in Microsoft's Visual C++ compiler.
  The change has no impact whatsoever on the Kakadu core system;
  it affects only the implementation of a few application level
  objects -- the core system abstracts all such I/O considerations
  through interface classes which are implemented by applications.
  Everything now runs a little faster than it did in version 2.1 and
  quite a bit faster than it did in the first release of version 2.2.
* Made provision for compiling under versions of GCC prior to version 3.0.
  To use this, you should define GCC_VERSION_LESS_THAN_3.

Changes from version 2.1 to version 2.2
---------------------------------------
* Extensive support for ROI (Region of Interest) specification
  at encode time (see "kakadu.pdf" for more on this).
* Migrated from the old-style C++ iostream package to the new standard
  iostream package -- minimal use of "using namespace std" and never used
  in common header files, so this should enhance namespace protection
  and portability.
* Added AT&T style versions of the small amount of  Pentium assembly code
  to enable compilation of a high speed version under GCC as well as
  MSVC.
* Some minor bug fixes.

Changes from version 2.0.2 to version 2.1
-----------------------------------------
* Extensive support for working with the JP2 file format.  The "kdu_show"
  application demonstrates the capabilities required of a conformant
  JP2 reader: palette mapping; interpolation of components to a common
  resolution; application of registration offsets in the CRG marker
  segment; and colour conversion to an appropriate rendering space (sRGB
  here).  The "kdu_region_decompressor" object provides extensive support
  for general purpose interactive rendering applications, performing all
  of the above tasks in a platform independent manner.
* It is now possible to directly control rate-distortion slope thresholds
  used in the construction of quality layers.  This capability may also
  be used to significantly increase compression speed, if a suitable
  threshold is known, since the encoder then incrementally predicts
  the point at which there is no point in coding further coding passes.
* A number of improvements to the "kdu_show" application, including
  the ability to arbitrarily zoom into images.
* A number of minor bug fixes, including one important bug reported by
  Aaron Deever of Kodak, and a bug which occasionally manifested itself
  in the incremental rate prediction heuristic (reported by Jim Andrew
  of CISRA).
* Improved documentation.

Changes from version 2.0.1 to version 2.02
------------------------------------------
* A PDF document (documentation.pdf) has been prepared to guide the new
  user into some of the more important aspects of the Kakadu system.  The
  first draft is included here.
* A very simple compression example and a very simple decompression example
  have been added to assist developers in familiarizing themselves with
  the Kakadu system -- the existing demo apps provide perhaps too much
  functionality to be quickly understood.
* A full BIBO (Bounded Input Bounded Output) numerical analysis of
  the DWT and other processing steps is used to establish the best
  utilization of limited precision sample data representations.
  The new version should not be able to fall prey to numerical
  overflow or underflow difficulties under any circumstances (this
  could just have been possible with the last version).  It also
  provides slightly higher accuracy.
* The automatic heuristic for generating quality layer rate
  targets has been substantially improved.
* A number of minor bugs/limitations were corrected, although these
  had not manifested themselves in any real examples.


Changes from version 2.0 to version 2.01
----------------------------------------
* One line change in each of "kdu_expand.cpp" and "kdu_compress.cpp" to
  correct a minor rare bug in these demo applications.
* Minor changes in "kdu_show.cpp" to correct a rare bug and improve the
  user interface in regard to image rotation.
* Four lines added to each of "encoder.cpp" and "decoder.cpp" to fix
  a minor memory leak.
